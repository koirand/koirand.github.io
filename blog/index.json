[
    {
        "ref": "https://koirand.github.io/blog/2021/airflow-oauth/",
        "title": "Apache AirflowでOAuth認証する",
        "section": "blog",
        "tags": null,
        "date" : "2021.02.17",
        "body": "Apache Airflowの認証時にKeyCloakを使ってOAuth認証する方法のメモ。\n環境  Keycloak v11.0.2 Airflow v2.0.0  authlibパッケージのインストール OAuth認証に必要なのでインストールする。コンテナで運用する場合はコンテナイメージをカスタムする必要がある。\npip install authlib  Airflow webserverの設定 ログインユーザーの情報を取得する際に、KeyCloakで保持している項目名とAirflowで保持している項目名に乖離があるため、マッピングしてやる必要がある。webserver_config.py で以下を設定する。\n# ${AIRFLOW_HOME}/webserver_config.py from airflow.www.security import AirflowSecurityManager from flask_appbuilder.security.manager import AUTH_OAUT class KeycloakSecurityManager(AirflowSecurityManager): def oauth_user_info(self, provider, response=None): if provider == 'keycloak': me = self.appbuilder.sm.oauth_remotes[provider].get('userinfo') return { 'username': me.json().get('preferred_username'), 'email': me.json().get('email'), 'first_name': me.json().get('given_name'), 'last_name': me.json().get('family_name') } else: return {} AUTH_TYPE = AUTH_OAUTH AUTH_USER_REGISTRATION = True AUTH_USER_REGISTRATION_ROLE = \u0026quot;Admin\u0026quot; keycloak_host = (your keycloak host) keycloak_relm_id = (your keycloak relm id) keycloak_client_id = (your keycloak client id) OAUTH_PROVIDERS = [{ 'name':'keycloak', 'token_key':'access_token', 'icon': 'fa-key', 'remote_app': { 'api_base_url': f'http://{keycloak_host}/auth/realms/{keycloak_relm_id}/protocol/openid-connect/', 'request_token_params': { 'scope': 'email profile' }, 'request_token_url': None, 'access_token_url': f'http://{keycloak_host}/auth/realms/{keycloak_relm_id}/protocol/openid-connect/token', 'authorize_url': f'http://{keycloak_host}/auth/realms/{keycloak_relm_id}/protocol/openid-connect/auth', 'client_id': keycloak_client_id, } }] SECURITY_MANAGER_CLASS = KeycloakSecurityManager  AirflowではSECURITY_MANAGER_CLASS でセキュリティマネージャーを指定できるようになっている1。\nデフォルトではFlask-AppBuilderのSecurityManagerを継承したAirflowSecurityManagerが使用される2。したがって更にそれを継承したKeycloakSecurityManagerを作成して、oauth_user_info関数を上書きすることで任意のマッピング処理を実装できる。\nAirflowではユーザー作成時にusername、email、first_name、last_nameの4つが必要なのでそれぞれ指定してやる。KeyCloak側でこれらの項目の値がNullだとログイン時にエラーになるので注意。\n  airflow/init_appbuilder.py at 2.0.0 · apache/airflow \u0026#x21a9;\u0026#xfe0e;\n airflow/security.py at 2.0.0 · apache/airflow \u0026#x21a9;\u0026#xfe0e;\n   "
    }
,
    {
        "ref": "https://koirand.github.io/blog/2021/vpn-traffic-setting/",
        "title": "すべてのトラフィックをVPN経由にするかどうかの設定",
        "section": "blog",
        "tags": null,
        "date" : "2021.01.26",
        "body": "Windows WindowsではデフォルトですべてのトラフィックがVPNを経由する。解除するには以下の設定を行う。\n コントロールパネル =\u0026gt; ネットワークとインターネット =\u0026gt; ネットワーク接続 に移動 VPNを右クリックしてプロパティを選択 ネットワークタブを開いて、インターネットプロトコルバージョン4(TCP/IPv4)を選択して、プロパティを選択 詳細設定を開く 「リモートネットワークでデフォルトゲートウェイを使う」のチェックを外す  MacOS MacOSではデフォルトではVPNのアドレス宛の通信のみVPNが使用される。すべてのトラフィックをVPN経由にするには以下の設定を行う。\n システム環境設定 =\u0026gt; ネットワーク に移動 VPNを選択して、詳細を選択 「すべてのトラフィックをVPN接続経由で送信」にチェックを入れる  "
    }
,
    {
        "ref": "https://koirand.github.io/blog/2021/kubernetes-tools/",
        "title": "Kubernetesの運用で普段使っているツール",
        "section": "blog",
        "tags": null,
        "date" : "2021.01.23",
        "body": "k9s https://github.com/derailed/k9s\nクラスタ管理用のTUIツール。UIがとても使いやすく、表示するリソースの種類やNamespaceの切り替えが簡単に素早くできる。Podのイベントやログの確認、ポートフォワードも簡単。さらにはマニフェストの定義を直接書き換えたり、ReplicaSetの数を変更したりといったことまでできる。正直これに慣れるとkubectlコマンドを打つのが苦痛になってくる。なくてはならないツール。\nkubectx https://github.com/ahmetb/kubectx\nKubernetesクラスタのコンテキストの切り替えが簡単になるツール。kubectl config get-contextや、kubectl config use-contextコマンドをわざわざ打たなくて良くなる。fzfコマンドとの連携がサポートされていて、fzfがインストールされているとコンテキストをインタラクティブに選択できるのだが、自分はfzfではなくpecoを使っているので、シェルに以下のエイリアスを追加して使っている。\n# kubectx alias kc=\u0026quot;kubectx | peco | xargs kubectx\u0026quot;  こうしておくとkcコマンドでpecoを使ったコンテキストの切り替えができる。\nkustomize https://kustomize.io/\nKubernetesのマニフェストの構成管理をするツール。Staging環境とProduction環境の環境差異などをこれで吸収できる。現在kustomizeコマンドはkubectlコマンドに組み込まれていて、kubectl apply -kで使えるようになっているが、実際に使ってみたところオリジナルのkustomizeコマンドよりもバージョンが低く使えない機能があったので、今の所オリジナルの方を使っている。\n似た用途として、Helmというツールもある。Helmを動かすにはtillerをクラスタにインストールする必要があり、それが面倒でkustomizeを使っていたのだが、最近リリースされたHelm v3ではtillerが廃止されて使いやすくなったので、kustomizeからHelmに乗り換えようかなと考えていたりする。\nRancher https://www.rancher.co.jp/\nクラスタ管理用のWebアプリ。とても使いやすいし、Kubernetesのリソースを作成する際に、マニフェストを書かなくてもフォームの要な箇所を埋めて登録するだけで作成できてしまうのが非常に簡単で便利。Kubernetesを一般の企業で普及させていくには、どうしてもハードルが高い場合があるが、Rancherを使うとそのハードルをいくらか下げることができると思う。SSO機能もあるのでユーザー管理も楽。\nPrometheus, Grafana https://prometheus.io/\nhttps://grafana.com/\nPrometheusとGrafanaをインストールするとKubernetesクラスタの利用状況がいい感じのダッシュボードで監視できるようになる。KubernetesにPrometheusをインストールするのはやや難しいので、kube-prometheusを使うのがおすすめ。kube-prometheusという名前だがGrafanaも一緒にインストールされる。もしくは、RancherにPrometheusとGrafanaをインストールする機能があるので、それを使ってインストールするのもあり。\nVerelo https://velero.io/\nKubernetesのメタデータやPersistent Volumeのバックアップ・リストアツール。運用の面倒臭いところがこのツールを使うことが圧倒的に楽になるのでとてもおすすめ。うっかりコンテキストを間違えてデプロイしてしまったり、Namespaceを削除してしまっても安心。Producton環境でKuberentesを使うなら必須だと思う。\nTrivy https://github.com/aquasecurity/trivy\nコンテナイメージの脆弱性チェックツール。UIが使いやすく、Go言語製で環境依存が小さくCIに組み込みやすいので使っている。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2021/keycloak-federation/",
        "title": "KeycloakでLDAP認証する",
        "section": "blog",
        "tags": null,
        "date" : "2021.01.18",
        "body": "KeycloakでLDAP認証をしてみたときのメモ。\n環境  Kubernetes v1.17.16 (minikube) Keycloak v11.0.2 がクラスタ上で稼働済み  OpenLDAPとphpLDAPadminを起動 以下のコンテナイメージを使用した。\n osixia/openldap - Docker Hub osixia/phpldapadmin - Docker Hub  Kubernetesマニフェストは以下。\napiVersion: v1 kind: Namespace metadata: name: ldap --- apiVersion: v1 kind: Service metadata: labels: app: openldap name: openldap namespace: ldap spec: ports: - name: ldap1 port: 389 - name: ldap2 port: 636 selector: app: openldap type: ClusterIP --- apiVersion: v1 kind: Service metadata: labels: app: phpldapadmin name: phpldapadmin namespace: ldap spec: ports: - name: https port: 443 selector: app: phpldapadmin type: ClusterIP --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: openldap name: openldap namespace: ldap spec: replicas: 1 selector: matchLabels: app: openldap template: metadata: labels: app: openldap spec: containers: - image: osixia/openldap:1.4.0 name: openldap --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: phpldapadmin name: phpldapadmin namespace: ldap spec: replicas: 1 selector: matchLabels: app: phpldapadmin template: metadata: labels: app: phpldapadmin spec: containers: - env: - name: PHPLDAPADMIN_LDAP_HOSTS value: openldap.ldap.svc.cluster.local image: osixia/phpldapadmin:0.9.0 name: phpldapadmin  ユーザー登録 登録例\n kubectl port-forward を使ってphpLDAPadminの画面にアクセス Login DN: cn=admin,dc=example,dc=org / Password: admin でログイン Generic: Organisational Unit を作成 (Unit name: People) Generic: Posix Group を作成 作成したOrganisational Unit(People)の子Entryとして、Generic: User Account を作成  KeycloakでのFederation設定 設定例 動作確認  phpLDAPadminで作成したユーザーを使ってログインできる Keycloakに同期されたユーザーは、Keycloak管理画面のManage =\u0026gt; Usersから確認できる  後片付け $ kubectl delete ns ldap  "
    }
,
    {
        "ref": "https://koirand.github.io/blog/2020/thrift-server/",
        "title": "Spark Thrift Server起動手順メモ",
        "section": "blog",
        "tags": null,
        "date" : "2020.08.26",
        "body": "Sparkに組み込まれているThrift Serverを起動した時の手順のメモ。\n環境  Spark3.0.0 (Pre-build for Apache Hadoop 3.2 and later) MySQL 5.7.24 (Hiveメタストアとして利用)  MySQLユーザー、データベース作成 MySQL側でhiveユーザーとmetastoreデータベースを作成する。Hiveにはデータベースを自動作成する機能があるが、それを使うとデータベースの文字コードがlatin1になってしまったので、マルチバイト\u0008ユーザーは手で作成したほうが良いと思われる。\nCREATE USER 'hive'@'%' IDENTIFIED BY 'hivepass'; CREATE DATABASE metastore DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; GRANT ALL ON metastore.* TO 'hive'@'%';  MySQL テーブル作成 これもデータベースと同様、Hiveにはテーブルを自動作成する機能があるのだが、バグっているのかうまく動かないためこれも手で作成した。Spark組み込みのHiveのバージョンが2.3.7だったため、GitHubの以下からDDLをダウンロードしてMySQL上で実行した。\nhive/hive-schema-2.3.0.mysql.sql at master · apache/hive\n実行する前にlatin1をutf8に、latin1_binをutf8_general_ciにそれぞれ置換した。また、utf8の場合インデックスに指定できるカラムの最大長が256バイトであるため、varchar(767)をvarchar(256)に置換した。\nMySQLドライバのインストール Mavenのリポジトリからダウンロードして所定の場所に置く。\ncurl -o ${SPARK_HOME}/jars/mysql-connector-java-8.0.21.jar https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.21/mysql-connector-java-8.0.21.jar  hive-site.xmlを作成 ${SPARK_HOME}/conf/hive-site.xml を作成して以下の設定を記述。\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionURL\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:mysql://{hostname}/metastore\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionDriverName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;com.mysql.jdbc.Driver\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionUserName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hive\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionPassword\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hivepass\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.schema.verification\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;  Thrift serverを起動 下記スクリプトを実行してThrift serverを起動する。(オプション省略)\n${SPARK_HOME}/sbin/start-thriftserver.sh \\ --name spark-thrift-server \\ --master \u0026lt;master-uri\u0026gt; \\ ...  懸念点１ hive.metastore.schema.verification オプションをfalseにしないと現状起動しない。falseにしてもログファイルに以下のログが出力される。恐らくメタストアのバージョンが取得できていないことが原因と思われるが、なぜ取得できないのかが不明。(情報求む)\nWARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0  懸念点２ Setting up hidden External Hive Metastore (MySQL),ADLS access configuration in Databricks Spark Cluster によると、lower_case_table_names 2に設定したほうが良いらしい。\n参考サイト  分散SQLエンジン - Spark 3.0.0ドキュメント Hiveテーブル - Spark 3.0.0ドキュメント 日本語訳 Spark - Hive との連携失敗 (バージョン非互換による Runtime Exception の発生) 非表示の外部Hiveメタストア（MySQL）のセットアップ、Databricks SparkクラスターでのADLSアクセス構成  "
    }
,
    {
        "ref": "https://koirand.github.io/blog/2020/about-blockchain/",
        "title": "ブロックチェーンの概要",
        "section": "blog",
        "tags": null,
        "date" : "2020.03.05",
        "body": "ブロックチェーン説明用のメモ。\nブロックチェーンとは  データが地理的に離れたサーバーに分散保持され、記録されたデータがなくならない（改竄不可能性）、 また一部のサーバーが不正侵入されても動き続ける（ビザンチン耐性）という特長を備えた全く新しいデータベースです。\nブロックチェーンとは - bitFlyer より\n 要するに、ブロックチェーンはデータベースの一種であるということ。\nブロックチェーンの特徴 ブロックチェーンは以下の特徴を持ったデータベースである。\n  改竄不可能性（Immutability）  各トランザクション（データ）は連続したブロックに格納されます。そのブロックに依存関係があるので、過去の一部を改竄した場合は、それ以降のトランザクションをすべて整合性がある形で改竄する必要があり、事実上不可能です。   ビザンチン耐性（Byzantine Fault Tolerance: BFT）  ビザンチンノード（嘘つきまたは故障したコンピューター）が一定数存在しても、ブロックチェーンは正しく動き続けます。   単一障害点（Single Point of Failure: SPOF）の排除  単一箇所が動かないと、システム全体が障害となる箇所。従来型のシステムではマスター、コントローラーや認証局等が単一障害点となります。ブロックチェーンに単一障害点はありません。    ブロックチェーンとは - bitFlyer より\n ブロックチェーンの仕組み Blockchain Demo このサイトがとても分かりやすい。\nこのサイトを使って日本語で解説している動画もある。\n  ブロックチェーンが改竄できない理由\n ひとつひとつのブロックにNonceが埋め込まれていて、そのNonceを計算するのが割と大変 ブロックがチェーンになっているので、一つのブロックを改竄すると後続のすべてのブロックのNonceを計算しないといけなくなる 多数決になっているので、自分のノードだけを更新しても認められない  "
    }
,
    {
        "ref": "https://koirand.github.io/blog/2020/vimmer-ahk/",
        "title": "WindowsのVimでEscキー押下時にIMEをオフにする",
        "section": "blog",
        "tags": null,
        "date" : "2020.02.25",
        "body": "VimでEscキーやCtrl+[キーを押してノーマルモードに戻ったときはIMEはオフになってほしいというのが、全Vimmerの願いだと思う。MacOSだと Karabiner-Elements を使う方法が有名だが、Windowsでのいいやり方がぱっとググっても見つからなかったので、専用のツールを作った。\nソースコード koirand/vimmer-ahk: Turn off IME when press Esc key or Ctrl+[ key\n使い方 Releases ページにあるvimmer-akh.exeを実行すると常駐するので、その状態でVimを使うと幸せになれる。 WSLでもIDEのVim拡張機能でも何でもござれ。 exeファイルを以下のフォルダに配置しておくと、Windows起動時に自動起動する。\n%APPDATA%\\Microsoft\\Windows\\Start Menu\\Programs\\Startup  仕組み AutoHotkey を使って作成した。 AutoHotkeyはキーバインドを自在にカスタムできるスクリプト言語で、スクリプトをコンパイルしてexeファイルにすることもできる。 下記サイトでAutoHotkeyでIMEを制御できる関数が提供されていたので有り難く拝借した。\n IME制御 - eamat @Cabinet - アットウィキ\n 拝借した関数を除くと、自分で書いたコードは下記の2行だけなので大したことはしていない。\n~Esc::IME_SET(0) ~^[::IME_SET(0)  似た事例として、Altキー単体でIMEをオン/オフするツールを見つけた。併用すると便利かも。\n karakaram/alt-ime-ahk\n "
    }
,
    {
        "ref": "https://koirand.github.io/blog/2020/spark-on-k8s/",
        "title": "PySpark on k8s (with Jupyter Notebook)",
        "section": "blog",
        "tags": null,
        "date" : "2020.02.25",
        "body": "Spark 2.4からサポートされた、Kubernetes上でのPySpark実行をやってみた。\n環境  MacOS 10.15.3 Docker Desktop 2.2.0.0  ※ Minikubeは未検証\n動作イメージ ソースコード koirand/spark-notebook-on-k8s-example\nk8sマニフェストとJupyter NotebookのExampleを格納してある。\nk8sクラスタへのデプロイ k8sマニフェストを作成してあるので、applyすれば動くはず。 メモリを結構食うので、事前にDocker Desktopの設定を変更してメモリのリソースを増やしておいたほうが無難。(4GBくらいあれば十分なはず)\n$ kubectl apply -f manifest.yaml  spark-nsのネームスペースにもろもろリソースが作成される。 Jupyter Notebookから実行したかったので、Driver PodにはPySparkが同梱された jupyter/pyspark-notebook - Docker Hub を使用している。 また、Executor Podの自動生成に使用するサービスアカウントも作成している。\nJupyter Notebookのアクセストークンを確認\nkubectl logs jupyter --namespace=spark-ns  Jupyter Notebookにポートフォワードで接続\nkubectl port-forward jupyter --namespace=spark-ns 8888  http://localhost:8888 にブラウザでアクセスすればJupyter Notebookが使える。\nPySparkの実行方法 example.ipynbをアップロードしてそのまま動かせば動くはず。\n後片付け kubectl delete -f manifest.yaml  ハマったところ バージョンの不一致 Driver PodとExecutor PodでPythonとSparkのバージョンを完全に一致させないと動かない。Driver Podに使用した jupyter/pyspark-notebook - Docker Hub はPython3.7.4だが、SparkのソースコードからビルドしたExecutor用のDockerイメージはPython3.7.3になってしまいバージョンが一致しない。\n結局、SparkのソースコードからビルドしたDockerイメージにPython3.7.4をインストールしたものを用意した。\nkoirand/spark-py: The patch for spark-py docker image.\nこれをExecutorに使うことでDriverとバージョンが一致する。\nDriver podとExecutor pod間の通信 Executor podからDriver podへのアクセスにjupyter-svc.spark-ns.svc.cluster.localという名前を使用している。\nconf.set(\u0026quot;spark.driver.host\u0026quot;, \u0026quot;jupyter-svc.spark-ns.svc.cluster.local\u0026quot;)  DriverのサービスがHeadlessの場合はDriverとExecutor間でうまく通信ができたが、Headlessでない場合はなぜか通信ができなかった。ポートを開けてみたり色々試行錯誤してみたものの、うまくいかず原因不明。\nServiceAccount k8sの環境によっては、defaultのServiceAccountではExecutorが起動できない。なのでExampleのマニフェストでは別途ServiceAccount(spark-sa)を作成して、adminロールをバインドし、そのサービスアカウントをDriver Podに付与している。\n参考サイト  Running Spark on Kubernetes - Spark 2.4.5 Documentation Running Spark with Jupyter Notebook \u0026amp; HDFS on Kubernetes Bernd\u0026rsquo;s Memory Leaks  "
    }
,
    {
        "ref": "https://koirand.github.io/blog/2020/hosts/",
        "title": "hostsファイルについて解説する",
        "section": "blog",
        "tags": null,
        "date" : "2020.02.20",
        "body": "hostsファイルとは ホスト名をIPアドレスに変換する際に参照する設定ファイルのこと。 拡張子は無し。\nhostsファイルの場所 hostsファイルが存在する場所は以下の通り。\n   OS 場所     Linux, Mac /etc/hosts   Windows(XP以降) C:\\WINDOWS\\system32\\drivers\\etc\\hosts   Windows(2000以前) C:\\WINNT\\system32\\drivers\\etc\\hosts    hostsファイルの働き コンピュータが他のコンピュータにアクセスする際にはIPアドレスが必要になる。 しかしIPアドレスというのは「216.239.33.100」のように記憶しづらい。 じゃあIPアドレスに覚えやすい別名を付けて、その別名でアクセスできれば便利だよねということになった。 実際コンピュータにはその仕組みが実装されていて、hostsファイルにホスト名とIPアドレスを書いておけば、自動的にホスト名をIPアドレスに変換してくれるようになっている。\n例えば自社のサーバの名前を「foo」とした場合、hostsファイルには以下のように記述する。\n210.148.160.10 foo  こうしておくことで、fooという名前でサーバにアクセスできるので、fooというサーバ名だけを覚えておけば良いことになる。 また、サーバのIPアドレスが変更になった場合も、hostsファイルだけを書き換えれば済むというメリットもある。 ちなみに、ホスト名をIPアドレスに変換することを「名前を解決する」と言う。\n例として、MacOSのデフォルトのhostsファイルは以下のようになっている。\n## # Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 127.0.0.1\tlocalhost 255.255.255.255\tbroadcasthost ::1 localhost  localhostというホスト名は、実はhostsファイルでIPアドレスに変換されていたのであった。\nhostsとDNSの違い DNSもhostsファイルと同じくホスト名とIPアドレスを変換するための仕組みである。 hostsファイルだと設定を他のコンピュータと共有したい場合にファイルの配布が面倒くさいのでそれを簡単にするために生まれた。 hostsファイルはコンピュータが自分でホスト名をIPアドレスに変換するのに対し、DNSでは専用のサーバを立てて、コンピュータがDNSに問い合わせてIPアドレスを取得する仕組みになっている。 コンピュータの台数が10台程度の小さなLANであればhostsで十分運用できるが、大規模のWANやインターネットでは一般的にDNSが使われている(といいつつも自分は昔3000台近い数のPCにhostsファイルを配信していたことがある。かなり大変だが無理ではない模様)。\nDNSはhostsファイルと比較して配布が簡単である一方で、みんながDNSを参照しているため自分の都合で気軽に変更しづらいというデメリットもある。 なので例えば「ちょっとテストのために自分だけ一時的にアクセス先を変えたい」みたいなケースであれば、自分だけhostsファイルに記載して向き先を変えるのが良い。 コンピュータはhosts→DNSの順番で名前解決を試みるため、hostsファイルとDNSに同じホスト名の設定があった場合はhostsファイルの設定が優先される。 ただ、hostsファイルに設定したことを忘れてしまって、後日「あれ？何故か繋がらないんだけど！？」という事態になるのはあるあるなので気をつけよう。\nhostsファイルを編集してみる hostsファイルを変更するためのコマンドは特にないので、普通にテキストエディタを使って変更する。 hostsファイルは通常一般ユーザーの権限では書き込みができないので、管理者権限が必要。\n192.168.3.1 foo 192.168.1.1 bar 192.168.3.2 baz  このように1つのホストに対して1行ずつ記載する。 変更できたら反映されているかpingコマンドで確認しよう。 (nslookupコマンドやdigコマンドはDNSの設定を確認するためのコマンドでありhostsファイルは使用されないので、ここではpingを使う)\n$ ping foo PING foo (192.168.3.1): 56 data bytes Request timeout for icmp_seq 0  適当なIPアドレスなので応答は無いが、192.163.3.1に変換できていることが分かる。\n余談 ちなみに、hostsファイルを改変して不正なサイトへ誘導するのはマルウェアの常套手段である。もしhostsファイルに覚えのない記述があればウィルスチェックをしたほうがいいかも？\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2020/gitconfig/",
        "title": ".gitconfigの設定を棚卸しした",
        "section": "blog",
        "tags": null,
        "date" : "2020.02.11",
        "body": "みんな大好きGit。先日、久々に自分の.gitconfigを見たら若干秘伝のタレ化していて、「なんでこの設定にしたんだっけ？？」状態になったので、設定を見直して棚卸ししてみた。Gitの設定に自信がない人は参考になるかも？\n見直した結果 こうなった\n[core] quotepath = false whitespace = cr-at-eol editor = vim [user] name = koirand email = koirand.jp@gmail.com [fetch] prune = true [pull] ff = only [merge] ff = false [push] default = current  ちなみに、この.gitconfigファイルはINI形式といわる形式なので、;でコメントが書けたりする。行頭のインデントはなくても動作するけど、git configコマンドで設定を行うとタブ文字でインデントされるので、それに倣うのがいいかと思われる。\n削除した設定 color.ui = true git diffとかした時に色付けする設定。デフォルトtrueだったので削除。\ncore.ignorecase = false ファイル名の大文字小文字の差分を無視する設定。デフォルトfalseだったので削除。\nというか、git initをすると、リポジトリのローカル設定(.git/config)がignorecase = trueになる(MacOS)。グローバルの設定よりもローカルの設定が優先されるので、実はグローバルにtrueを設定しようがfalseを設定しようが、あまり意味がない。ファイル名の大文字小文字を無視したい場合は、リポジトリ個別に設定を変更しないといけない。なかなかの罠。\n$ git config core.ignorecase false  filemode = false ファイルのパーミッションの変更を無視する設定。 これもignorecaseと同じく、git initをすると、リポジトリのローカル設定(.git/config)がfilemode = trueになる(MacOS)。なのでグローバルに設定してもあまり意味がないので消した。 Unix系の端末で作成したリポジトリをWindows端末で開いたりなどすると、ファイルのパーミッションが自動的に変更されて、それが差分として検知されてしまうことがあるらしいので、その場合はWindows端末上で、\n$ git config core.filemode true  とやるとパーミッションの変更がGit上で検出されなくなる。 ただこれをやると、いざシェルスクリプトなどに実行権限を付与したい時に困る。そんなときは、\n$ git update-index --add --chmod=+x {ファイル名}  とやるといける。\ncore.autocrlf = false 改行コードを自動的に変換しないようにする設定。デフォルトfalseだったので削除。\nuser.useConfigOnly user name、emailが指定されいないとコミットできないようにする設定。user name、emailともに設定ファイルで明示的に設定してあるので特に不要。\ncredential.helper = cache \u0026ndash;timeout=86400 HTTPでの認証情報を記憶しておく時間の設定。SSHしか使っていないので削除。\n残った設定 core.quotepath = false 日本語ファイル名の文字化けを防ぐ。日本人なので残す。\ncore.whitespace = cr-at-eol 行末のCRを許容する。Windows端末で触る時のために残す。最近Windowsで開発することが殆どなくなってきたけど、一応。\ncore.editor = vim コミットやマージした時に起動するエディタ。Ubuntuとかでたまにnanoが起動してうざいので設定しておく。OSのデフォルトのエディタを変更したほうがいいかもだけど、コンテナとかで毎回やるの面倒くさいし。\nuser.name, user.email まあこれは要るよね。\nfetch.prune = true リモートで削除されたブランチをローカルで自動的に削除する設定。プルリクがマージされた時、ローカルのブランチをいちいち削除しなくて良くて済む。普通に便利なので残す。\npull.ff = only git pull --ff-onlyをデフォルトにする設定。pull時に変なマージコミットを作りたくないので設定している。普通にやってたらマージコミットは発生しないはずだけど念の為。\nmerge.ff = false git merge --no-ffをデフォルトにする設定。さっきのpull.ffとは逆に、マージする時はマージコミットを必ず作りたいマンなので設定している。そうすることで、コミットログのどこからどこまでがトピックのまとまりなのか分かりやすくなる。\npush.default = current git push origin HEADをデフォルトにする設定。カレントブランチと違う名前のリモートブランチにプッシュすることがこれまで無かったので問題なし。\nおわり。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2020/getting-start-docker/",
        "title": "Docker入門",
        "section": "blog",
        "tags": null,
        "date" : "2020.02.05",
        "body": "社内でDocker入門講座を開催した時に作った資料。\n 公式ドキュメント  Docker Documentation Docker ドキュメント日本語化プロジェクト  Dockerとはなんぞや  Dockerは仮想化技術の一つ(VirtualBOX, HyperVと同じくくり) OS(カーネル)の上にコンテナ層を作ることを発明した Linuxしか動かせないが、その代わり軽くて速い コンテナはDockerイメージ(isoイメージみたいなやつ)をもとに生成できる 色々なDockerイメージがDockerHubで公開されている  何が嬉しいの？  起動が早くてキビキビ動くので、取り回しやすい リソース消費量が少ないので、1プロセス1コンテナみたいな世界が可能になる イメージのサイズが小さいのとDockerHubがあるおかげで公開がとても楽  インストール 公式サイトでDocker Desktopのインストーラーをダウンロードしてインストールしてください。\nWindowsにDocker toolboxをインストールしている記事を見かけることがありますが、2020年2月時点ではDocker toolboxは廃止されて非推奨となっているようです。\n基本的なコマンド一覧 結構種類があって大変ですが、この後ひとつひとつ実行して動作確認していきます。\nコンテナ操作系    コマンド 用途     docker run コンテナを生成して起動する   docker exec コンテナ内でコマンドを実行する   docker ps コンテナの状態を参照する   docker stop コンテナを停止する   docker start コンテナを起動する   docker rm コンテナを削除する   docker logs コンテナのログを参照する    Dockerイメージ操作系    コマンド 用途     docker images Dockerイメージの一覧を参照する   docker rmi Dockerイメージを削除する   docker commit コンテナに対して行った変更をDockerイメージとして保存する   docker build DockerfileからDockerイメージを作成する   docker login Dockerイメージのリポジトリサイトにログインする   docker push リポジトリにDockerイメージを格納する    他にも色々ありますので、気になる人はdocker --helpで確認してみてください。\nチュートリアル１（コンテナを操る） チュートリアル１ではdockerコマンドを実際に叩いてみて、Dockerに慣れていきましょう。\nNginxをDocker上で起動する 以下の通りdocker runコマンドを実行してください。\n$ docker run -d -p 8080:80 --name=nginx nginx:latest Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx bc51dd8edc1b: Pull complete 66ba67045f57: Pull complete bf317aa10aa5: Pull complete Digest: sha256:ad5552c786f128e389a0263104ae39f3d3c7895579d45ae716f528185b36bc6f Status: Downloaded newer image for nginx:latest 6a4692e3ff5f601b1c81c7a720888251b5b7ac3fe0d02bb90de736488a35f28b  docker runを実行すると、DockerHubからDockerイメージがダウンロードされ、それを使ってコンテナが生成されます(docker run のオプションについてはマニュアルを参考にしてください)。http://localhost:8080 にアクセスするとNginxが起動しているのが確認できるかと思います。\nコマンド内のnginx:latestという部分がDockerイメージの識別子になります。Dockerイメージは{リポジトリ名}:{タグ}で一意に識別されるので覚えておいてください。latestタグは最新バージョンのDockerイメージに付与されるタグです。もしdocker run実行時にタグを省略した場合は、latestタグが使用されます。\nコンテナの状態を確認してみる docker imagesでDockerイメージの一覧を表示できます。\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE nginx latest 2073e0bcb60e 44 hours ago 127MB  Dockerイメージの一覧にnginxが追加されているかと思います。これが先程docker run実行時にDockerHubからダウンロードされたDockerイメージです。\n次にコンテナの状態をdocker psで確認しましょう。\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4e3aa2d731f3 nginx:latest \u0026quot;nginx -g 'daemon of…\u0026quot; 3 seconds ago Up 2 seconds 0.0.0.0:8080-\u0026gt;80/tcp nginx  Nginxのコンテナが作成されていて、STATUSがUp(稼働中)であることが分かります。\nNginxのコンテナ内でBashを実行する コンテナ内で何かしらのコマンドを実行したいときは、docker execを使います。\n$ docker exec -it nginx /bin/bash root@4e3aa2d731f3:/#  Nginxのコンテナ内で実行されたBashに繋がったかと思います。ここでls -lコマンドでNginxのログフォルダを確認してみてください。\nroot@4e3aa2d731f3:/# ls -l /var/log/nginx total 0 lrwxrwxrwx 1 root root 11 Feb 2 08:06 access.log -\u0026gt; /dev/stdout lrwxrwxrwx 1 root root 11 Feb 2 08:06 error.log -\u0026gt; /dev/stderr  ログファイルではなく、標準出力、標準エラー出力に繋がれたシンボリックリンクがありました。基本的にコンテナの世界ではログは標準出力および標準エラー出力に出力するのが習わしとなっています。そうすると、出力した文字列がDockerによってホストOSに転送され、docker logsコマンドで参照することができるようになります。Nginxコンテナもこれに倣ってログを標準出力、標準エラー出力に吐くよう作られているわけです。では一旦exitでシェルを抜けて、docker logsコマンドを実行してみましょう。\nNginxのログを見てみる $ docker logs nginx 172.17.0.1 - - [04/Feb/2020:03:48:15 +0000] \u0026quot;GET /robots.txt HTTP/1.1\u0026quot; 404 555 \u0026quot;-\u0026quot; \u0026quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36\u0026quot; \u0026quot;-\u0026quot; 2020/02/04 03:48:15 [error] 6#6: *1 open() \u0026quot;/usr/share/nginx/html/robots.txt\u0026quot; failed (2: No such file or directory), client: 172.17.0.1, server: localhost, request: \u0026quot;GET /robots.txt HTTP/1.1\u0026quot;, host: \u0026quot;localhost:8080\u0026quot; 172.17.0.1 - - [04/Feb/2020:03:48:15 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36\u0026quot; \u0026quot;-\u0026quot; 172.17.0.1 - - [04/Feb/2020:03:48:16 +0000] \u0026quot;GET /favicon.ico HTTP/1.1\u0026quot; 404 555 \u0026quot;http://localhost:8080/\u0026quot; \u0026quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36\u0026quot; \u0026quot;-\u0026quot; 2020/02/04 03:48:16 [error] 6#6: *2 open() \u0026quot;/usr/share/nginx/html/favicon.ico\u0026quot; failed (2: No such file or directory), client: 172.17.0.1, server: localhost, request: \u0026quot;GET /favicon.ico HTTP/1.1\u0026quot;, host: \u0026quot;localhost:8080\u0026quot;, referrer: \u0026quot;http://localhost:8080/\u0026quot;  先程アクセスしたログが確認できるかと思います。ちなみにdocker logs -fとしてやると、tail -fのようにログの追記を監視することができます。\nコンテナを停止して起動する コンテナを停止するときはdocker stopです。\n$ docker stop nginx nginx  コンテナが停止し、http://localhost:8080 にアクセスできなくなっているはずです。docker psで状態を確認します。\n$ docker ps docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES  Nginxのコンテナが表示されなくなりました。ただしNginxのコンテナは停止しているだけで削除されたわけではありません。停止しているコンテナも含めて表示するにはdocker ps -aとやります。\n$ docker ps -a docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4e3aa2d731f3 nginx:latest \u0026quot;nginx -g 'daemon of…\u0026quot; About an hour ago Exited (0) 2 minutes ago nginx  このようにNginxのコンテナが表示され、STATUSがExitedであることが確認できます。このコンテナを再度起動するにはdocker startを実行します。\n$ docker start nginx  もう一度docker psコマンドを叩くと、またNginxコンテナが表示され、STATUSがUp(稼働中)であることが分かります。\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4e3aa2d731f3 nginx:latest \u0026quot;nginx -g 'daemon of…\u0026quot; About an hour ago Up 23 seconds 0.0.0.0:8080-\u0026gt;80/tcp nginx  コンテナを削除する コンテナの削除はdocker rm です。ただし、起動中のコンテナは削除できませんので、いったんdocker stopで停止してから削除してください。\n$ docker stop nginx nginx $ docker rm nginx nginx  これでコンテナが削除されるため、docker ps -aを叩いてもNginxは表示されません。\n$ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES  Dockerイメージを削除する 先程コンテナは削除しましたが、Dockerイメージはまだ残っています。後片付けとしてDockerイメージも削除しましょう。docker rmiコマンドで削除できます。\ndocker rmi nginx:latest Untagged: nginx:latest Untagged: nginx@sha256:ad5552c786f128e389a0263104ae39f3d3c7895579d45ae716f528185b36bc6f Deleted: sha256:2073e0bcb60ee98548d313ead5eacbfe16d9054f8800a32bedd859922a99a6e1 Deleted: sha256:a3136fbf38691346715cac8360bcdfca0fff812cede416469653670f04e2cab0 Deleted: sha256:99360ffcb2da18fd9ede194efaf5d4b90e7aee99f45737e918113e6833dcf278 Deleted: sha256:488dfecc21b1bc607e09368d2791cb784cf8c4ec5c05d2952b045b3e0f8cc01e  この状態でdocker imagesコマンドを叩くと、NginxのDockerイメージが削除されていることが確認できるかと思います。\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE  以上でチュートリアル１は完了です。\nここまでの知識で、DockerHub上で公開されているDockerイメージが利用できるはずです。最近はOSSの配布をDockerで行うことが増えてきました。Dockerを使うと複雑なアプリケーション(３層アーキテクチャとか)でも簡単にインストールできます。また、コンテナはホストOSの環境と切り離されているため、ローカルPCの環境が汚れてしまう恐れもありません。コンテナを削除すると環境もろとも綺麗に削除されます。この気軽さがDockerを使う大きなメリットの一つです。DockerHubには様々なDockerイメージが公開されているので、ぜひ色々動かしてみてください。\nチュートリアル２ （Dockerイメージをカスタマイズする） dockerコマンドに慣れたら、次のステップとしてオリジナルのDockerイメージの作成に挑戦してみましょう。チュートリアル２ではPythonの開発環境用にDockerイメージを作成します。\nDocker HubでPythonを検索 DockerHubにアクセスします。アカウントを持っていない人はサインアップしてください。Pythonで検索すると、PythonのDockerイメージが見つかるかと思います。\nDockerHubには、公式のイメージだけでなく個人が作った信頼性の低いDockerイメージも公開されています。DockerHubを使う際は、「VERIFIED PUBLISHER」や「OFFICIAL IMAGE」のラベルがあるかどうかや、ダウンロード数、スター数を参考にして信頼性を確認するようにしましょう。\nではPythonを選択して、Tagsをクリックしてタグを表示してください。\n様々なタグがあることが確認できるかと思います。Pythonの場合、Python自体のバージョンと、OSのバージョンでタグが付けられているようです。ここでは3.8.1-busterを使うことにします。busterというのはDebianというOSのバージョン10のコードネームです。つまり、Debian10にpythonがインストールされた状態のDockerイメージということになります。\nコンテナを起動 docker runでコンテナを起動します。名前はmypythonにしました。Dockerイメージのサイズが大きいので割と時間がかかります。\n$ docker run -d -it --name mypython python:3.8.1-buster Unable to find image 'python:3.8.1-buster' locally 3.8.1-buster: Pulling from library/python dc65f448a2e2: Pull complete 346ffb2b67d7: Pull complete dea4ecac934f: Pull complete 8ac92ddf84b3: Pull complete a3ca60abc08a: Pull complete 9253bd2ee3f6: Pull complete fad96c8dce44: Pull complete ec0f51d2752d: Pull complete ff10001c6dc3: Pull complete Digest: sha256:db86894e8fcdb6ca371e1143cbc646ef554ca83c5a85dc4a13f427e085153bb5 Status: Downloaded newer image for python:3.8.1-buster 39c933d7c9a31329b8be6e3aaba62f1454f7089844b2351da5c050a3ffff5e61  起動できたら、コンテナ内のBashに接続して、Pythonのバージョンを確認してみます。\ndocker exec -it mypython /bin/bash root@35c8bacc3411:/# python --version Python 3.8.1 root@35c8bacc3411:/# pip --version pip 20.0.2 from /usr/local/lib/python3.8/site-packages/pip (python 3.8)  ちゃんとバージョン3.8.1がインストールされているのが確認できます。\nコンテナをカスタマイズ このコンテナを開発環境用にカスタマイズしていきます。なんでも良いですが、今回は開発にPandasが必要になったという想定で、Pandasをインストールしてみます。\nroot@35c8bacc3411:/# pip install pandas Collecting pandas Downloading pandas-1.0.0-cp38-cp38-manylinux1_x86_64.whl (9.9 MB) |████████████████████████████████| 9.9 MB 10.1 MB/s Collecting numpy\u0026gt;=1.13.3 Downloading numpy-1.18.1-cp38-cp38-manylinux1_x86_64.whl (20.6 MB) |████████████████████████████████| 20.6 MB 591 kB/s Collecting pytz\u0026gt;=2017.2 Downloading pytz-2019.3-py2.py3-none-any.whl (509 kB) |████████████████████████████████| 509 kB 10.0 MB/s Collecting python-dateutil\u0026gt;=2.6.1 Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB) |████████████████████████████████| 227 kB 11.2 MB/s Collecting six\u0026gt;=1.5 Downloading six-1.14.0-py2.py3-none-any.whl (10 kB) Installing collected packages: numpy, pytz, six, python-dateutil, pandas Successfully installed numpy-1.18.1 pandas-1.0.0 python-dateutil-2.8.1 pytz-2019.3 six-1.14.0 root@35c8bacc3411:/# python Python 3.8.1 (default, Feb 2 2020, 08:37:37) [GCC 8.3.0] on linux Type \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information. \u0026gt;\u0026gt;\u0026gt; import pandas \u0026gt;\u0026gt;\u0026gt; exit() root@35c8bacc3411:/#  インストールできました。次にこのコンテナを使って新しいmypythonという名前のDockerイメージを作成します。exitでシェルを抜けて、docker commitを実行します。\nxxxxxの箇所はご自身のDockerHubのユーザー名に置き換えてください。\n$ docker commit mypython xxxxx/mypython:3.8.1-buster sha256:8e8066959b691698077fc78988dbc09d77a7e586b010e690f9e22ed093da9b80 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE xxxxx/mypython 3.8.1-buster 80db86c8f1ae 5 seconds ago 1.1GB python 3.8.1-buster eeadc22d21a9 47 hours ago 933MB  xxxxx/mypythonという名前のDockerイメージが作成されているのが確認できます。\nコンテナをDockerHubで公開する このmypythonをDockerHubで公開して、他の人とシェアできるようにしてみます。それほど難しい操作は必要なく、docker loginコマンドでDockerHubにログインし、docker pushするだけで簡単に公開できます。\n$ docker login (ご自身のID・パスワードでログインしてください) $ docker push xxxxx/mypython:3.8.1-buster  これで、DockerHubのマイページにmypythonが表示されているはずです。DockerHubにアクセスして確認してみましょう。\nこのようにDockerイメージ化することで、開発環境をチームで共有することが可能になります。Dockerが実行環境の差異を吸収してくれるので、もしチームの中にWindowsユーザーとMacユーザーとLinuxユーザーが混在していたとしても環境差異でハマることが殆どありません。環境差異に悩まなくて良いのもDockerの大きなメリットの一つです。\nDockerイメージを作成するもう一つの方法 先程行ったDockerイメージのカスタマイズでは、\n カスタマイズのベースとなるDockerイメージを使ってコンテナを起動 コンテナ内のBashに接続 何かしらをインストール docker commitコマンドでオリジナルのDockerイメージを作成  という流れでDockerイメージを作成しましたが、それとは別にdocker buildコマンドを使う方法もあります。docker buildを使った方法では、まずDockerfileを作成します。(Dockerfileというファイル名です。拡張子はありません。)\n以下がDockerfileの例です。上記の手順の1〜3をスクリプトで自動化するようなイメージです。\nFROM python:3.8.1-buster RUN pip install pandas  そして、Dockerfileのあるフォルダ上で、docker buildコマンドを実行すると、xxxxx/mypythonという名前のDockerイメージが自動的に作成されます。\n$ docker build -t xxxxx/mypython:3.8.1-buster . Sending build context to Docker daemon 300.6MB Step 1/3 : FROM python:3.8.1-buster ---\u0026gt; eeadc22d21a9 Step 2/3 : RUN pip install pandas ---\u0026gt; Running in 290d7be35a90 Collecting pandas Downloading pandas-1.0.0-cp38-cp38-manylinux1_x86_64.whl (9.9 MB) Collecting numpy\u0026gt;=1.13.3 Downloading numpy-1.18.1-cp38-cp38-manylinux1_x86_64.whl (20.6 MB) Collecting python-dateutil\u0026gt;=2.6.1 Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB) Collecting pytz\u0026gt;=2017.2 Downloading pytz-2019.3-py2.py3-none-any.whl (509 kB) Collecting six\u0026gt;=1.5 Downloading six-1.14.0-py2.py3-none-any.whl (10 kB) Installing collected packages: numpy, six, python-dateutil, pytz, pandas Successfully installed numpy-1.18.1 pandas-1.0.0 python-dateutil-2.8.1 pytz-2019.3 six-1.14.0 Removing intermediate container 290d7be35a90 ---\u0026gt; e67c3fa0fb05 Step 3/3 : ENTRYPOINT /bin/bash ---\u0026gt; Running in 16b989c8b647 Removing intermediate container 16b989c8b647 ---\u0026gt; f50c90e2b7df Successfully built f50c90e2b7df Successfully tagged xxxxx/mypython:3.8.1-buster  この方法はDockerイメージの作成を自動化できるため、何度もDockerイメージを作成する必要がある場合に向いています。一般的にCIツールと組み合わせて自動化することが多いです。Dockerfileの構文を覚える必要があるので今回は時間的に解説できませんが、興味がある方は、Dockerfile リファレンス、Best practices for writing Dockerfiles などを読んでみてください。\n以上でチュートリアル２は完了です。\nDocker Tips Dockerに関するTipsをいくつか紹介します。\ndocker runのオプション 以下のオプションを常に付けることをおすすめします。\n   オプション 意味     -d バックグラウンドで実行   \u0026ndash;rm コンテナ停止時に自動的にコンテナを削除   \u0026ndash;name {名前} コンテナに名前を付ける    -dを常に付ける理由は、Dockerコンテナを不用意に停止させないためです。例えば、-dを付けずに実行すると、\n$ docker run -it --name mypython python:3.8.1-buster /bin/bash root@da3508c54ee2:/# exit exit $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES  というように、プロセスが終了するとコンテナが停止してしまいます。デタッチ/アタッチという仕組みもありますが、それをやるなら-dを使うほうが簡単です。\n$ docker run -d -it --name mypython python:3.8.1-buster 13b59f84fc3d68f53c244e0801c336f6f439dce75a359ac1b5f05e96753c1bd9 $ docker exec -it mypython /bin/bash root@13b59f84fc3d:/# exit exit $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 13b59f84fc3d python:3.8.1-buster \u0026quot;/bin/bash\u0026quot; 9 seconds ago Up 7 seconds mypython  このように、コンテナのシェルにはdocker execで接続します。この方法だとシェルを抜けてもコンテナは停止しません。停止したいときはdocker stopで明示的に停止します。\n--rmを付ける理由としては、コンテナを停止状態で置いておく必要がないからです。なのでコンテナを停止したら勝手に削除してくれたほうがコンテナを削除する手間が省けます。コンテナの中で作成したファイルを消したくないというケースはあるかと思いますが、それであればdocker run時にホストOSのフォルダをマウントしてホストOS側に保存するか、もしくはdocker commitでDockerイメージ化したほうが安全です。「コンテナを誤って削除してしまった」という事故を防げます。\n--nameを付ける理由は、これをしないとDockerに割り当てられたランダムな単語を使ってコンテナの操作をしなければならなくなるからです。\n$ docker run -d -it --rm python:3.8.1-buster 783aab3c9c46dda9ad44e8289871f1ac0594698ef85799b93da43587b357f336 d $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 783aab3c9c46 python:3.8.1-buster \u0026quot;/bin/bash\u0026quot; 1 second ago Up Less than a second romantic_rhodes  というように、この場合ではromantic_rhodesという名前か、もしくはCONTAINER IDを使わなければならず、操作性が低くなります。\ndocker runのコマンドが長くて見づらい オプションが多くなった場合は、バックスラッシュで改行すると見やすいです。 （Windowsはキャレットで改行できる？）\n$ docker run \\ -d \\ -it \\ --rm \\ --name mypython \\ python:3.8.1-buster  Dockerイメージのゴミを削除したい Dockerを使っていると、Dockerイメージのゴミが溜まってきます。そんなときはdocker system pruneとやると掃除できます。たまに実行すると良いかと思います。\n$ docker system prune WARNING! This will remove: - all stopped containers - all networks not used by at least one container - all dangling images - all dangling build cache Are you sure you want to continue? [y/N] y Deleted Images: untagged: koirand/mypython@sha256:09973f3e3a51d59d5d57306b928a3c24ee4cbd8baeda46ab60e3810e38c1d9d4 deleted: sha256:80db86c8f1ae595cedd7d909156bd0638e21ab7c6b6da48af57fff58bdd30e31 deleted: sha256:70eca39dc93885416684888ad63f2e68133c848e0708ffba0681d5938cfb6936 deleted: sha256:142edfeafc60e404b5038812ede95e8a5fb902b6e041231d9e0304eab7127a37 deleted: sha256:ef94e002c5727977623f12810fc8e8c45c8c89d80ec1d613c059e85b222bc61f deleted: sha256:3f70246ee63ab87ba8717df29a81c7b5eb0812493da7e03b0c23363ec3513661 deleted: sha256:f5e1b6e88039e92cad924d47fafe4c0c533914cf2d8d136396f47e22e99a6f62 deleted: sha256:7d31605ba9a91ad8c70438b2dbb8fee6f599b165137f03a58b918274dbd106eb deleted: sha256:930f81dcf389890495ca1103ab4361718b4dc51a1d149b88454424566e76293b deleted: sha256:899090de47ef18ae38c91752ae5d3d91bcea683c0b7a1610d92a56a61f82e694 deleted: sha256:014bf9ec8a38021ca309be775d9bbd55be782417b03e79b53fda9e43dc1261d4 deleted: sha256:05e4bef6d74ab30ceda651deaf8cf5098493ae4b3637f80cc84d0d4d39e008a8 deleted: sha256:a2e778d0eb88ab6a02ff378eb3605d9bdf34fef5545427a22312bb0cb85729ef deleted: sha256:f1c46cc442566e473088a0cd5ea74e6da8bcbe4456f041e461e721d7c2f29e8c deleted: sha256:66e1bbe79ff4646f032b7aeaa8acda8bf28fbac489dc55a403cb2d025a56ee9a deleted: sha256:96c1612de1b3dea0b223faafdbde1553e17697468b5951456d338105bf208de3 Total reclaimed space: 447.5MB  VSCode拡張機能 VSCodeの便利な拡張機能を紹介します。\nDocker - Visual Studio Marketplace これを入れると、ツールバーにDockerアイコンが表示され、コンテナの操作をGUIで行うことができます。(さっき紹介したゴミの削除もできます。)\nRemote - Containers - Visual Studio Marketplace これはまだプレビューですが、VSCodeからコンテナに直接アクセスできる革命的な拡張機能です。\nエイリアス 毎回dockerを打つのがダルいという人はエイリアスを設定すると良いかと思います。\nalias d=\u0026quot;docker\u0026quot;  以上、Docker入門講座でした。お疲れさまでした。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2020/two-factor-auth/",
        "title": "二要素認証が面倒くさい",
        "section": "blog",
        "tags": null,
        "date" : "2020.01.22",
        "body": "二要素認証を設定したクラウドサービスにログインする際にスマホアプリ(Google Authenticatorなど)を使うのが割と一般的だが、これがまあ面倒くさい。\n スマホを取り出す スマホのロックを解除する Google Authenticatorを立ち上げる 該当のアカウントを探す パスコードを見ながらPCで入力  となかなか手数が多い。二要素認証のアカウントは増える一方だし、毎日ログインが必要なものもある。もっと楽したい。\nパスワードマネージャーを使う 意外と知られてないが最近のパスワードマネージャーはTOTPに対応している。自分はBitwardenを使っている。BitwardenでTOTP機能を使うには有料ライセンスを購入する必要があるが、年間1000円くらいで安い。Bitwardenの場合、ID/PASS認証後、自動的にクリップボードにパスコードがコピーされるのでペーストするだけで済む。とても楽ちん。\noathtoolを使う 場合によってはコマンドラインツールを使ったほうが楽な場合もある。Macの場合は、\n$ brew install oath-toolkit  でoauthtoolをインストールし、\n$ oathtool --totp --base32 {シークレット文字列}  とやることでパスコードが出力される。\nalias totp=\u0026quot;oathtool --totp --base32 {シークレット文字列} | pbcopy\u0026quot;  としておくと、totpコマンドでパスコードがクリップボードにコピーされる。\nAutomatorを使う 更にAutomatorを使うと、キーボードショートカットにコマンドを割り当てられるようになる。\nこんな感じでクイックアクションを作成して、\nシステム環境設定 -\u0026gt; キーボード -\u0026gt; ショートカット で割当を行う。\nこうすることで、ショートカットキーを押すだけでパスコードがクリップボードにコピーされる。とても楽ちん。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2020/bunsenlabs/",
        "title": "BunsenLabsインストールメモ",
        "section": "blog",
        "tags": null,
        "date" : "2020.01.03",
        "body": "10年以上前に買ったDEL Inspiron Mini9が押し入れで眠っていたので、超軽量LinuxのBunsenLabsをインストールしてみた。流石にスペックが貧弱なので、とりあえずテキストエディタで日本語の文章が書けてChromeで何かしらに投稿できれば十分ということにした。\nインストール BunsenLabs Linux :: Installation でisoファイルをダウンロードできるのでそれをUSBメモリに焼いてインストールした。今どきi386 no PAEをサポートしているのは凄い。タッチパネルや無線LANなどのハードも、何もせずにちゃんと認識された。\n初期セットアップ 初回起動時には、自動的に bl-welcome が起動するので、ウィザードに従って進むと必要なソフトウェアがインストールされる。OSのバージョンは以下になった。\n$ cat /etc/debian_version 9.11 $ cat /etc/bunsen/bunsen_install Install method: iso Version: helium-5-cd-i386 Disk info: Debian GNU/Linux 9.8 _Helium_ - Snapshot i386 LIVE/INSTALL Binary 20190706-20:38 Install date: 2020-01-03T06:28:45 Added bunsen-meta-all at 2020-01-03T07:42:05, version 9.0.2-1  パッケージ追加 最低限のパッケージをインストール\n$ sudo apt install git vim chromium fcitx-mozc  Dotfiles追加 $ mkdir ~/Projects $ git clone git@github.com:koirand/dotfiles.git ~/Projects/dotfiles $ ln -s ~/Projects/dotfiles/.gitconfig ~/.gitconfig $ ln -s ~/Projects/dotfiles/.vimrc ~/.vimrc $ ln -s ~/Projects/dotfiles/.vim ~/.vim $ ln -s ~/Projects/dotfiles/.Xmodmap ~/.Xmodmap  デフォルトアプリ変更 vimとChromeがデフォルトで起動するようにする。\n$ update-alternatives --all  Openboxのautostart変更 CapsLockキーをCtrlキーとして使うための設定だけ。 ~/.config/openbox/autostart に追記\nxmodmap ~/.Xmodmap  Fcitxの設定 EscキーとCtrl+[でIMEを切る設定だけ。\nsed -i -e 's/#InactivateKey=/InactivateKey=ESCAPE CTRL_[/' ~/.config/fcitx/config  終わり 画面が9インチしかなくてとても小さいので、作業中はFn + F11で最大化すると良いかと。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2019/vim-memo/",
        "title": "Vimでさっとメモを取る",
        "section": "blog",
        "tags": null,
        "date" : "2019.10.29",
        "body": "Vimさっとメモを取るいい感じの方法を発見したので紹介。まずシェルで以下のエイリアスを設定する。\nalias memo=\u0026quot;vim + ~/Documents/memo.txt\u0026quot;  ポイントは+をつけていることで、こうすると最下行にカーソルがある状態でファイルを開くことができる。 次に.vimrcに以下を設定する。\nautocmd BufNewFile,BufRead memo.txt $r! echo '--------------------------------------------------------------------------------' \u0026amp;\u0026amp; date  これはどういう意味かというと、「memo.txtを開いたときに、区切り線と日付をバッファの最終行に追記しなさい。」という意味になる。r!でシェルコマンドの実行結果をバッファに取り込むことができる。\n以上の設定により、ターミナルでmemoと打てばmemo.txtが開き、区切り線と日付が追記され、さらに最終行にカーソルがある状態になる。そこからoをタイプしてメモを書いていく。 Vimは起動が非常に速いし、自分はたいていターミナルを常時起動しているのもあって、かなり素早くメモを書き始めることができる。Vimmerにはおすすめ。\nデモ "
    }
,
    {
        "ref": "https://koirand.github.io/blog/2019/create-react-app-v3/",
        "title": "create-react-app v3がリリースされたのでTypeScriptに移行してみた",
        "section": "blog",
        "tags": null,
        "date" : "2019.05.18",
        "body": "2019年４月にv3.0.0がリリースされた。\nRelease v3.0.0 · facebook/create-react-app\n Jest 24に対応 Hooksをサポート TypeScriptのLintに対応 browserslistをサポート jsconfig.json/tsconfig.json を使用した絶対位置インポートに対応  が追加機能となっている。 個人的にLintの設定変更がめんどくさくてTypeScriptに移行していなかったところがあるので、これを機にTypeScriptへの移行をやってみた。\nリポジトリ こちらを題材とした。 https://github.com/koirand/nixie-timer\ncreate-react-appのバージョンアップ プロジェクトフォルダで\n$ yarn add react-scripts  とやると、現時点で最新版のv3.0.1が入った。\nTypeScriptに移行してみる TypeScriptをインストール $ yarn add typescript @types/node @types/react @types/react-dom @types/jest  jsファイルの拡張子をtsxに変更 拡張子はtsではなくtsxにする必要がある模様。 テストコードも含めて機械的にやるのみ。\ntsconfig.jsonを作成 yarn start するとtsconfig.jsonが自動的に生成される。\n\u0026quot;noImplicitAny\u0026quot;: false  だけ追記して(修正がめんどくさかった)、その他はデフォルトのままとした。\nコンパイルエラー修正 https://github.com/koirand/nixie-timer/pull/41/commits/9dcdaf07df3bb9d732432ea851289c99c82116f6\nエラーの修正に加えて、PropTypesを使わない変更をしたり、ステータスの定数をenumにしたりした。\n以上で、TypeScriptへの移行終わり。\nVSCodeの設定変更 ESLintプラグインがデフォルトだとjsファイルしか見てくれないので、tsxファイルも見るように以下の設定を追加した。\n{ \u0026quot;eslint.validate\u0026quot;: [ \u0026quot;javascript\u0026quot;, \u0026quot;javascriptreact\u0026quot;, { \u0026quot;language\u0026quot;: \u0026quot;typescript\u0026quot;, \u0026quot;autoFix\u0026quot;: true }, { \u0026quot;language\u0026quot;: \u0026quot;typescriptreact\u0026quot;, \u0026quot;autoFix\u0026quot;: true } ] }  感想 コンパイルエラーは意外と少なかった(2,3件くらい)。慣れれば数分で移行できそうなレベル。 プロトタイプはES6で書いて、商用のタイミングでTypeScriptに移行するのが良さそう。\n個人的にIDEはAtom推しなのでAtomでの設定方法も調べてみたが、linter-eslintプラグインでtsxファイルを見るようにする方法が良くわからなかった。Reactの公式サイトでもVSCodeを推奨するとか書いてあったりするし、FlutterとかもVSCodeしかサポートしていなかったりするので、もうVSCodeでいいやという気になってきた。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2019/cloud9/",
        "title": "AWS Cloud9を使ってみた",
        "section": "blog",
        "tags": null,
        "date" : "2019.05.02",
        "body": "前回、AWS IoTボタンを使うためにAWSアカウントを作ったので、ついでにクラウドIDEであるCloud9を触ってみた。Cloud9については、re:Invent2017の動画がわかりやすい。\nAWS re:Invent 2017 - Introducing AWS Cloud9: Werner Vogels Keynote - YouTube\nクラウドIDEに求めるもの これまでにクラウドIDEを使ったことはない。技術書の写経とかで環境構築がめんどくさかったりするので(Pythonとか)、そのへんがサクッとできると嬉しい。その他には以下の機能があればいいなという感じ。\n Vimキーバインドが使える Terminalが使える Lint、自動修正ができる コード補完ができる コードの共同編集ができる(社内勉強会などで使いたい)  セットアップ Cloud9を動かすホストとしてEC2を使うのが一般的だと思うけど、自分は個人でさくらのVPS(Ubuntu)を契約しているのでそこで動かすことにした(AWS良くわからない)。ホスト側でやったことは以下の通り。\n cloud9用にユーザーを作成する(一般ユーザーで問題なし)  既存のユーザーでも問題ないと思うけど一応別ユーザーにした   AWSコンソールに表示される公開鍵を.ssh/authorized_keysに登録 Node.jsをインストール(今回はv10.15.3)  動いた。\n対応している言語 ここに記載されている。\nAWS Cloud9 Integrated Development Environment (IDE) の言語のサポート - AWS Cloud9\nC++も対応しているので、競プロをやるのも面白いかと思った。\nHello, world Goのサンプルコードを動かしてみた。\nAWS Cloud9 の Go サンプル - AWS Cloud9\n.bashrcにパスを設定\nGOPATH=$HOME/go PATH=\u0026quot;/usr/local/go/bin:$GOPATH/bin:$PATH\u0026quot;  .goファイルを開くと、エラーになる。\n調べるとどうもgocodeが必要らしいので、gocodeをインストールした。\ngo get github.com/nsf/gocode  動いた。\n補完はこんな感じ。\nコードの補完にgocodeを使っているので、goのバージョンは1.9以下にしておいたほうが良いと思われる。goのコード補完はlspへの移行期なので、1.10以降はもう少し落ち着いてから使おう。mattnさんが以下の記事で経緯を説明してくれている。\nBig Sky :: gocode やめます(そして Language Server へ)\nフォルダ階層はチュートリアルと違うけど、一般的な開発だとこんな感じになるかと思う。 Vimキーバインド 普段良く使うキーはだいたい問題なかった。ウィンドウ操作系はやはり難しいようだった。\nLambda Functionとの連携 前回作ったLambda Function をCloud9上で変更してみる。\n以下を参考にした。\nAWS Cloud9 Integrated Development Environment (IDE) で AWS Lambda 関数を操作する - AWS Cloud9\nLambdaへのアクセス権限を持つcloud9用のIAMを作成して、aws-cliをインストールすると、Remote Functionsが見えるようになる。\n関数をインポートしようとしたらエラーが。goのランタイムはサポートしてないっぽい。まあコンパイル言語だし無理か。残念。\n感想 好きな言語の好きなバージョンの仮想環境がサクッと立ち上がるのかなとか勝手に思ってたけど、そういうものではないらしい。クラウドIDEという名前の通りあくまでIDEなので、環境は自分で構築する必要がある。環境汚したくない場合はコンテナを使ったほうが良さそう。環境構築をサクッとやりたいのであればPiaza Cloudとかのほうが良いのかもしれない。とはいえ一度環境を作ってしまえば、どのPCでも開発できてしまうのは熱い。IDEとしては普通に使いやすい。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2019/slack-timeclock/",
        "title": "AWS IoTボタンで作る出退勤ボタン",
        "section": "blog",
        "tags": null,
        "date" : "2019.05.01",
        "body": "前々から触ってみたかったAWS IoTボタンをついに購入した。\nAmazonで2500円で販売している。Dash buttonが500円だったので、やや割高に感じる。\nAmazon.co.jp： AWS IoT エンタープライズボタン – IoT をシンプルに。\n先日Slack Incoming Webhookを使ってSlackに通知する技術を会得したので、今度は出勤・退勤時にこのボタンを押てSlackに通知するようにしたら、タイムカードみたいに使えて便利じゃないかな？と思って作ってみた。\nソースコード koirand/slack-timeclock: Lambda function to use AWS 1-click button as an timeclock button.\nボタンをクリックすると出勤、ダブルクリックすると退勤の通知が飛ぶ。便利。\nところでタイムカードは英語圏ではどう呼んでいるんだろうか、TimeClockとしてみたけど通じるのか若干不安がある。たぶん通じるはず。\nAWS Lambda Function LambdaはGo言語にも対応しているので今回もGoで書いてみた。またLambdaは環境変数をセットできるので、SlackのWebhook URLは環境変数に設定することにした。 ビルドとデプロイは以下のようにやって、できたzipをLambdaにアップロード。\nGOOS=linux GOARCH=amd64 go build zip slack-timeclock.zip slack-timeclock  AWS IoT1-Click IoTボタンの設定はスマホでやるのが簡単だった。\nAWS IoT 1-Click モバイルアプリ - AWS IoT 1-Click\nボタンを押したときのアクションは、Eメールを送信、SMSを送信、Lambda関数を実行の３つから選べる。デバイスを登録してWifiに繋いだあと、プロジェクトを作成してLambda関数を選択すれば連携される。プレイスメントというところで、デバイスごとに異なる属性を設定することができる。今回は自分の名前を設定して、Slackのメッセージに名前が組み込まれるようにしてみた。\nこうすることで、他の人用にもう一つボタンを用意した際に、Lambda関数は同じものを使用して、属性だけ変えれば済む。\nちなみに、このIoTボタンはAWSと8443ポートで通信するらしい。会社のFWでポートが閉じられている場合は穴あけしないと使えないので注意。\nシングルクリック、ダブルクリックの判別 シングルクリックかダブルクリックかの判別方法が公式のドキュメントを読んでも良くわからなかった。AWS難しい。試行錯誤の末、下記を参考にしてうまくいった。\n IoTButton Go Firebaseで「イラッと」をロギングする。 slay-t/merry  追記 2020/04/26 その後少し改良して、アプリからではなくて自分のアカウントから出退勤のメッセージが投稿されるようにした。こんなかんじ。\n自分のアカウントからメッセージを投稿するためには、Webhook URLを叩くだけのアプリではなくて、OAuthに対応したアプリを作らないといけないのでやや面倒くさいが、既存のアプリのトークンを使い回すことで楽ができる。\n自分は、slackcat というアプリを使った。 slackcatはターミナルなどからSlackに投稿できるCLIツール。 このアプリをPCにインストール後、slackcat --configureを実行してSlackのワークスペースで認可すると、~/.slackcatにトークンが保管されるのでそのトークンを使って、メッセージを投稿するコードを書けばOK。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2019/slack-balance/",
        "title": "口座残高をSlackに通知する",
        "section": "blog",
        "tags": null,
        "date" : "2019.04.30",
        "body": "GWだし何か作ろうということで、銀行の口座残高をスクレイピングしてSlackに通知というのをやってみた。\nソースコード koirand/slack-balance: Get bank balance and send to slack\n自分はMUFGをメインバンクとして使っているので、MUFGダイレクトのサイトにログインして口座残高を取得するように作ってみたものの、どうせなら他の口座の残高も見たいなあと作ってから気づいた。そこでMoneyForwardの口座ページにアクセスして、MoneyForwardと連携している口座残高の取得もできるようにした。ただ、MoneyForwardの残高はリアルタイムではないので、MUFGの処理は残しておくことにした。\nchromedp 今どきSelenium使うのもいけてないので、chromedpを使うことにした。chromedpはChrome DevTools Protocolを使ってChromeを自動操作するツール。Go言語でサクッと書ける。\nchromedp/chromedp: A faster, simpler way to drive browsers supporting the Chrome DevTools Protocol.\nExampleを見るとだいたいやり方がわかるかと。\nchromedp/examples: chromedp code examples.\nExampleのevalを見ると、\nchromedp.Evaluate(`Object.keys(window);`, \u0026amp;res)  のように、どうやら任意のJSの戻り値を取得することもできる模様。 また、Exampleのheadersを実行するとリクエストヘッダが見れる。Macで実行したらUserAgentは以下となっていた。\n\u0026quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/74.0.3729.108 Safari/537.36\u0026quot;  HeadlessChromeでアクセスしていることがサイト運営者には分かるようなので一応注意されたし。\nSlack Incoming Webhook Slackへの通知はIncoming Webhooksを使って実装した。実装といってもWebサイト上でWebhook URLを発行して、そこにPOSTすれば良いだけなので拍子抜けするほど簡単だった。\nSlack API | Slack\nこんな感じで通知できてる。良い。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2019/nixie-timer/",
        "title": "ニキシー管風のカウントダウンタイマーを作った",
        "section": "blog",
        "tags": null,
        "date" : "2019.04.06",
        "body": "「今年はunstatedがくる」と同僚が言っていたのでunstatedを使ってなにか作ってみようと思った。unstatedはReactのContextを使ってStateを簡単に管理できるモジュール。ソースコードは200行に満たないにも関わらずとても強力。\n jamiebuilds/unstated: State so simple, it goes without saying\n 作ったもの koirand/nixie-timer: Nixie tube style count down timer.\nデモは こちら 。時間を設定して[Start]ボタンを押すとカウントダウンが始まる。カウントダウン中は時計以外が隠れるので、デザインスプリントやワークショップなどで、スクリーンとかモニタとかに映すと格好がよろしいかと。画像の素材は Photoshop ニキシー管風味カウンター素材 - trismegistuslabo トリメギ から拝借させてもらった。\nunstated使ってみた感想 今いる会社はフロントエンドのエンジニアがそんなにいないのでReduxを社内で使っていくのは正直厳しいが、unstatedなら覚えることが少なくて大丈夫そうだと感じた。componentDidMountなどでStateを更新したい場合にHOCをかまさないといけないのが若干アレだけど慣れれば大したことない。\ngh-pages 今回 gh-pages - npm を使ってデモページをデプロイしてみた。このモジュールもとてもお手軽で良かった。\n\u0026quot;scripts\u0026quot;: { \u0026quot;predeploy\u0026quot;: \u0026quot;yarn build\u0026quot;, \u0026quot;deploy\u0026quot;: \u0026quot;gh-pages -d build -m \\\u0026quot;[ci skip]\\\u0026quot;\u0026quot; }  package.jsonにこのようにscriptsを書いておけば、yarn deployするとGitHub Pagesにデプロイできる。ちなみにCIをスキップするために、-mオプションでコミットメッセージに[ci skip]を入れるようにしている。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2019/git-zoo/",
        "title": "Gitのログを動物園にする",
        "section": "blog",
        "tags": null,
        "date" : "2019.04.06",
        "body": "Gitのログに絵文字のプレフィックスを入れている人をたまに見かける。バグだったら :bug: とか、テストだったら :white_check_mark: とか。見やすくていいんだけど、なんか仕事のルールって感じがして、もっと自由で無意味で楽しく開発したいと思った。例えば動物の絵文字を追加するとか。\n自動で絵文字を追加できたりしないのかなと思って調べたら、Git hookというものがあることを知った。\n Git - Git フック\n prepare-commit-msgでゴニョゴニョすることで、コミットメッセージを書き換えられるらしいので、ランダムに動物の絵文字を追加するツールを作ってみた。\n koirand/git-zoo: Add animals emoji to git commit message.  使い方はREADMEにある通り、適用したいGitリポジトリの.git/hooks/prepare-commit-msgにシンボリックリンクを作成すると、コミット時にコミットメッセージの行頭にランダムな動物の絵文字が挿入されるようになる。\nこんな風に楽しい感じになる。会社でやるとたぶんウザがられるのでオススメはしない。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2019/react-eslint/",
        "title": "create-react-appで作ったアプリをESLintする",
        "section": "blog",
        "tags": null,
        "date" : "2019.03.11",
        "body": "Sider というGitHubのプルリクエストを自動でレビューしてくれるサービスがあり、これをちょっと使ってみようと思った。 以前 React開発環境構築(create-react-app利用) に書いたとおり、ESLintの設定がダルいので普段はStandard Styleのモジュールを入れてLintの設定は簡単に済ましているが、このSiderがStandard Styleには対応してなくてJSHintかESLintにしか対応していないようなので、今回ESLintでLintしてみた。\nモジュールのインストール react-create-appにはReactをESLintするために必要なモジュールは同梱されている。なのでStandard Style用のプラグインだけ追加で入れる。\nyarn add -D \\ eslint-config-standard \\ eslint-plugin-node \\ eslint-plugin-standard \\ eslint-plugin-promise \\ prettier-standard  .eslintrc.json こいつが面倒\u0026hellip;かと思いきや下記の設定だけで良い模様。\n{ \u0026quot;extends\u0026quot;: [\u0026quot;react-app\u0026quot;, \u0026quot;standard\u0026quot;] }  これで yarn run eslint src/ などやればLintしてくれる。\nVimの設定(ALE) ALEはLintのモジュールを自動検知してくれるので、ESLintになっても設定は変更しなくても問題なかった。\nlet g:ale_fixers = {'javascript': ['prettier_standard']} let g:ale_fix_on_save = 1  Atom reactプラグインとlinter-eslintを入れればOK。 linter-eslintの設定をいじれば自動修正もしてくれる。\nVSCode eslintプラグインを入れればOK。 設定をいじれば自動修正もしてくれる。\nまとめ 思ったより設定項目少なくて楽だった。最初からこうすれば良かった感がある。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2019/macos-setting/",
        "title": "Mac初期設定メモ",
        "section": "blog",
        "tags": null,
        "date" : "2019.03.03",
        "body": "MacOS設定  システム環境設定  一般  書類を閉じるときに変更内容を保持するかどうかを確認 にチェック   Dockとメニューバー  Dockを左に表示 「最近使ったアプリケーションをDockerに表示」をオフ バッテリー ｰ\u0026gt; 割合（％）を表示   Mission Control  「最新の使用状況に基づいて操作スペースを自動的に並び替える」をオフ   キーボード  キーリピートとリピート入力認識までの時間をMAXに ショートカット -\u0026gt; 入力ソース を無効に ショートカット -\u0026gt; キーボード -\u0026gt; 次のウィンドウを捜査対象にする を`⌘``に変更   トラックパッド  「強めのクリックと触覚フィードバック」をオフ   Apple ID -\u0026gt; iCloud  Macを探す以外のチェックを解除   インターネットアカウント  Googleアカウント登録   アクセシビリティ  ポインタコントロール -\u0026gt; トラックパッドオプション -\u0026gt; 3本指のドラッグを有効     Finder  環境設定  サイドバー  全てチェック   詳細  拡張子を表示 30日後にゴミ箱から項目を削除     表示  タブバーを表示 パスバーを表示      dotfiles 事前にGoogle DriveからSSH鍵をダウンロードして.sshディレクトにいれておく。\n$ git clone git@github.com:koirand/dotfiles.git ~/dotfiles $ cd ~/dotfiles $ make install  Homebrew $ /bin/bash -c \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\u0026quot; $ brew bundle --global  fish shell $ sudo sh -c \u0026quot;echo /usr/local/bin/fish \u0026gt;\u0026gt; /etc/shells\u0026quot; $ chsh -s /usr/local/bin/fish  terminfo $ cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt;/tmp/xterm-256color-italic.terminfo # A xterm-256color based TERMINFO that adds the escape sequences for italic. xterm-256color-italic|xterm with 256 colors and italic, sitm=\\E[3m, ritm=\\E[23m, use=xterm-256color, EOF $ tic /tmp/xterm-256color-italic.terminfo   Italic fonts in iTerm2, tmux, and vim - Alex Pearce\n cheetsheet $ git clone git@gist.github.com:07d9a3195a38d5a14e636bdbec0e0862.git ~/Documents/cheetsheet  アプリインストール Apple Storeから  BitWarden Slack Skitch BetterSnapTool  Mac起動時に自動起動 Keyboard Shortcuts     Webサイトから  Brave Google Chrome Google 日本語入力 Karabiner Clipy LICEcap AppCleaner iTerm2  General -\u0026gt; Preferences  ~/dotfiles/.iterm2 を読み込む`     VS Code Docker Adobe Creative Cloud Spotify  Update 2020.12.30 Big Sur向けに修正\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2019/bitbucket-to-github/",
        "title": "ブログのリポジトリをGitHubに移行した",
        "section": "blog",
        "tags": null,
        "date" : "2019.03.02",
        "body": "以前、ブログ(HUGO)のビルドとデプロイをCircleCIで自動化した に書いた通り、ブログのリポジトリはBitbucketで管理して、ブログをビルドした結果生成されるhtmlファイルはGitHubにデプロイしていたが、先日GitHubのプライベートリポジトリが無料化されたのでGitHubだけで完結するようにリポジトリを移行してみた。\nBefore After CircleCIでのSSHキーの登録 特に難しいところはなかったが、SSHキーの登録だけ少しハマったのでメモ。 CircleCIのプロジェクト設定にあるPERMISSIONS -\u0026gt; Checkout SSH keys からSSHキーを登録する際、デプロイキーを追加する方法と、ユーザーキーを追加する方法の２種類がある。 最初にデプロイキーを選択したがうまく動かなかった。デプロイキーだとブログ用のリポジトリ(図の左上のリポジトリ)のRead権限しか付与されないので、GitHub Pages用のリポジトリ(図の右上のリポジトリ)に対してデプロイできる権限を付与できなかった。 一方ユーザーキーだと、全てのリポジトリにRead/Write権限を付与でき、これだとうまくデプロイできた。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2018/create-react-app/",
        "title": "React開発環境構築(create-react-app利用)",
        "section": "blog",
        "tags": null,
        "date" : "2018.12.10",
        "body": "Reactアプリの開発環境を一から構築するのは中々辛いものがある。たくさんモジュールを入れないといけないし、それらのモジュールの依存関係にも気を配らないといけない。ビルドやlintの設定を書くのも面倒。しかし今はcreate-react-appという便利なコマンドがあるようなので使ってみた。\n公式ドキュメント Create React App · Set up a modern web app by running one command.\nアプリケーションの作成 $ yarn global add create-react-app $ create-react-app my-app yarn create v1.12.3 [1/4] 🔍 Resolving packages... [2/4] 🚚 Fetching packages... [3/4] 🔗 Linking dependencies... [4/4] 📃 Building fresh packages... success Installed \u0026quot;create-react-app@2.1.1\u0026quot; with binaries: - create-react-app [################################################################] 64/64 Creating a new React app in /Users/kazuki/Projects/my-app. Installing packages. This might take a couple of minutes. Installing react, react-dom, and react-scripts... yarn add v1.12.3 [1/4] 🔍 Resolving packages... [2/4] 🚚 Fetching packages... [3/4] 🔗 Linking dependencies... [4/4] 📃 Building fresh packages... success Saved lockfile. success Saved 6 new dependencies. info Direct dependencies ├─ react-dom@16.6.3 ├─ react-scripts@2.1.1 └─ react@16.6.3 info All dependencies ├─ babel-preset-react-app@6.1.0 ├─ react-dev-utils@6.1.1 ├─ react-dom@16.6.3 ├─ react-error-overlay@5.1.0 ├─ react-scripts@2.1.1 └─ react@16.6.3 ✨ Done in 9.55s. Initialized a git repository. Success! Created my-app at /Users/kazuki/Projects/my-app Inside that directory, you can run several commands: yarn start Starts the development server. yarn build Bundles the app into static files for production. yarn test Starts the test runner. yarn eject Removes this tool and copies build dependencies, configuration files and scripts into the app directory. If you do this, you can’t go back!   yarn create | Yarn\n ちなみに、TypeScriptを使う場合は、\n$ create-react-app my-app --typescript  と--typescriptをつければ良いらしい。\nファイル構成 my-app ├── .git ├── .gitignore ├── README.md ├── node_modules ├── package.json ├── public | ├── favicon.ico | ├── index.html | └── manifest.json ├── src | ├── App.css | ├── App.js | ├── App.test.js | ├── index.css | ├── index.js | ├── logo.svg | └── serviceWorker.js └── yarn.lock  READMEやserviceWorker、manifest.jsonも生成してくれて、gitの初期化までしてくれる。至れり尽くせり。以下利用上の注意点。\n public/index.htmlとsrc/index.jsはリネーム、削除してはいけない ビルド時の処理対象はsrcフォルダ配下のみ ビルド後の静的ファイルが参照できるのはpublicフォルダ配下のみ  package.json Babelやwebpackのややこしいところがreact-scriptsで抽象化されている。\n{ \u0026quot;name\u0026quot;: \u0026quot;my-app\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.1.0\u0026quot;, \u0026quot;private\u0026quot;: true, \u0026quot;dependencies\u0026quot;: { \u0026quot;react\u0026quot;: \u0026quot;^16.6.3\u0026quot;, \u0026quot;react-dom\u0026quot;: \u0026quot;^16.6.3\u0026quot;, \u0026quot;react-scripts\u0026quot;: \u0026quot;2.1.1\u0026quot; }, \u0026quot;scripts\u0026quot;: { \u0026quot;start\u0026quot;: \u0026quot;react-scripts start\u0026quot;, \u0026quot;build\u0026quot;: \u0026quot;react-scripts build\u0026quot;, \u0026quot;test\u0026quot;: \u0026quot;react-scripts test\u0026quot;, \u0026quot;eject\u0026quot;: \u0026quot;react-scripts eject\u0026quot; }, \u0026quot;eslintConfig\u0026quot;: { \u0026quot;extends\u0026quot;: \u0026quot;react-app\u0026quot; }, \u0026quot;browserslist\u0026quot;: [ \u0026quot;\u0026gt;0.2%\u0026quot;, \u0026quot;not dead\u0026quot;, \u0026quot;not ie \u0026lt;= 11\u0026quot;, \u0026quot;not op_mini all\u0026quot; ] }  create-react-appに含まれるもの $ npm ls --depth=1 my-app@0.1.0 ├── node-pre-gyp@0.10.3 extraneous ├─┬ react@16.6.3 │ ├── loose-envify@1.4.0 │ ├── object-assign@4.1.1 │ ├── prop-types@15.6.2 │ └── scheduler@0.11.3 ├─┬ react-dom@16.6.3 │ ├── loose-envify@1.4.0 deduped │ ├── object-assign@4.1.1 deduped │ ├── prop-types@15.6.2 deduped │ └── scheduler@0.11.3 deduped └─┬ react-scripts@2.1.1 ├── @babel/core@7.1.0 ├── @svgr/webpack@2.4.1 ├── babel-core@7.0.0-bridge.0 ├── babel-eslint@9.0.0 ├── babel-jest@23.6.0 ├── babel-loader@8.0.4 ├── babel-plugin-named-asset-import@0.2.3 ├── babel-preset-react-app@6.1.0 ├── bfj@6.1.1 ├── case-sensitive-paths-webpack-plugin@2.1.2 ├── chalk@2.4.1 ├── css-loader@1.0.0 ├── dotenv@6.0.0 ├── dotenv-expand@4.2.0 ├── eslint@5.6.0 ├── eslint-config-react-app@3.0.5 ├── eslint-loader@2.1.1 ├── eslint-plugin-flowtype@2.50.1 ├── eslint-plugin-import@2.14.0 ├── eslint-plugin-jsx-a11y@6.1.2 ├── eslint-plugin-react@7.11.1 ├── file-loader@2.0.0 ├── fork-ts-checker-webpack-plugin-alt@0.4.14 ├── fs-extra@7.0.0 ├── fsevents@1.2.4 ├── html-webpack-plugin@4.0.0-alpha.2 ├── identity-obj-proxy@3.0.0 ├── jest@23.6.0 ├── jest-pnp-resolver@1.0.1 ├── jest-resolve@23.6.0 ├── mini-css-extract-plugin@0.4.3 ├── optimize-css-assets-webpack-plugin@5.0.1 ├── pnp-webpack-plugin@1.1.0 ├── postcss-flexbugs-fixes@4.1.0 ├── postcss-loader@3.0.0 ├── postcss-preset-env@6.0.6 ├── postcss-safe-parser@4.0.1 ├── react-app-polyfill@0.1.3 ├── react-dev-utils@6.1.1 ├── resolve@1.8.1 ├── sass-loader@7.1.0 ├── style-loader@0.23.0 ├── terser-webpack-plugin@1.1.0 ├── url-loader@1.1.1 ├── webpack@4.19.1 ├── webpack-dev-server@3.1.9 ├── webpack-manifest-plugin@2.0.4 └── workbox-webpack-plugin@3.6.3  だいたい必要なやつは入っている。PrettireやCSS in JSは標準では入っていないようなので、使いたい場合は自分で入れる必要がある。\nLint、Prettire .eslintrcや.prettierrcを書くのはとてもダルいので、Standard Style専用のモジュールを入れてしまう。\nyarn add -D standard prettier-standard    JavaScript Standard Style sheerun/prettier-standard: (✿◠‿◠) Prettier and standard brought together!   Macで開発する場合、標準で入っているtidyのバージョンが古く、HTML5に対応していないので、tidy-html5を入れておくと良い。\n$ brew install tidy-html5  Vimの設定 以下の設定にすると、保存時に自動的に修正されるようになる。楽チン。\nlet g:ale_fixers = {'javascript': ['prettier_standard']} let g:ale_fix_on_save = 1  Atom 以下２つのプラグインを入れればOK\n linter-js-standard standard-formatter  VSCode StandardJS プラグインを入れればOK\nまとめ 思ったより使いやすくて気に入った。webpack.config.jsを書かなくて済むのは大変良い。facebookのオフィシャルツールなので安心感がある。同じようなコンセプトのモジュールにParcelというものもあるので、React以外の場合は使ってみると良いかもしれない。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2018/package-manager/",
        "title": "パッケージマネージャーのチートシート",
        "section": "blog",
        "tags": null,
        "date" : "2018.11.30",
        "body": "前回の設定ファイルの書式と同様、パッケージマネージャーのコマンドもチートシートを作ってみた。コマンドのhelpで確認はできるが、パッケージマネージャーによってコマンドの意味が微妙に違っていたりするのでチートシートで目的からコマンドの逆引きしたほうが安心できる。自分用なので自分が使うものだけとはいえapt(Debian)、yum(Red Hat)、Homebrew(Mac OS)、npm(JavaScript)、yarn(JavaScript)、pip(Python)、dep(Golang)と結構種類が多い。パッケージマネージャーとはやや異なるがfisherman(fish)、Helm(Kubernetes)もついでなので一緒に記載しておいた。\n   Use Case apt(旧) apt(新) yum Homebrew npm yarn pip dep fisherman Helm     パッケージ追加 apt-get install apt-install yum install brew install npm install yarn add pip install dep ensure fisher install helm install   パッケージ削除 apt-get remove apt remove yum remove brew uninstall npm uninstall yarn remove pip uninstall dep remove fisher rm helm delete   リポジトリ更新 apt-get update apt update - brew update - - - - - helm update   パッケージ更新 apt-get upgrade apt upgrade yum update brew upgrade npm update yarn upgrade pip install -U - fisher update helm upgrade   パッケージ検索 apt-cache search apt search yum search brew search npm search - pip search - - helm search   パッケージ詳細 apt-cache show apt show yum info brew info npm view yarn info pip show - - helm inspect   追加したパッケージの確認 dpkg -l dpkg -l yum list installed brew list npm ls yarn list pip list dep status fisher ls helm list   ゴミパッケージの掃除 apt-get autoremove apt autoremove yum autoremove - npm prune yarn install - dep prune - -    Homebrewにautoremoveが無くて辛いことが良くあって、何か方法はないかと調べたらスクリプトをつくっている人がいた(しかもfish)。ありがたく拝借した。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2018/ini-json-yaml-toml/",
        "title": "INI,JSON,YAML,TOMLのチートシート",
        "section": "blog",
        "tags": null,
        "date" : "2018.11.19",
        "body": "設定ファイルの書式についてググることが多かったので自分用にチートシートを作成してみた。設定ファイルの規格で思いつくのは、INI、XML、JSON、YAML、TOMLの５種類。他にもあるだろうが私には接点がない。INIは物心ついたころには既にあった気がする。Windows 95のアプリケーションの設定ファイルがINIだった。それからphp.iniの印象が強い。今でも.gitconfigなどで触れる機会がある。XMLはJAVAのweb.xmlが最初の出会いだった。当時からなんでこんな扱いづらいフォーマットで書くのか理解できなかった。XMLが嫌でJAVAやApacheを使いたくないところがある。今でも古めのAPIはXMLだったりするし、SAMLなんかもXMLだ。JSONとの出会いは正直覚えていないが、何かしらのREST-APIだと思われる。Node.jsのpackage.jsonで良く触れる。YAMLはRubyを使ってこなかったのでこれまで接点が無かったが、最近Kubernetesを使い始めて良く触るようになった。TOMLはGolangを使った時に初めて触れた。Go言語界隈ではTOMLが良く使われている。HUGOの設定もTOMLだ。\n以上唐突な自分語り。何が言いたいのかというと、TPOに合わせて色々な規格を読み書きしないといけなくてややこしいと言うことだ。\nついでなので各規格の登場年月を調べた。\n   規格 初版     INI -   XML 1998年   YAML 2001年5月   JSON 2006年7月   TOML 2013年2月    意外とYAMLが古い。INIはWikipediaによると規格化・標準化されていないらしい。今初めて知った。自然発生的に生まれたと言うことだろうか。不思議な書式だ。そういう意味ではTOMLはINIを規格化したものと言えるのだろうか。ちなみに各フォーマットをGoogleトレンドだとこんな感じ。\nチートシート というわけでチートシート。(XMLは書かない)\nINI ; コメント key1=value1 [section] key2=value2 key3=1.1  INIに配列やネストを記述する方法はない。\nJSON { \u0026quot;key1\u0026quot;: \u0026quot;value1\u0026quot;, \u0026quot;key2\u0026quot;: { \u0026quot;key3\u0026quot;: null, \u0026quot;key4\u0026quot;: [ 1.1, 2.2 ] }, \u0026quot;key5\u0026quot;: [ { \u0026quot;key6\u0026quot;: 3, \u0026quot;key7\u0026quot;: true }, { \u0026quot;key6\u0026quot;: 4, \u0026quot;key7\u0026quot;: false } ] }  JSONにコメントはない。\nYAML # コメント key1: value1 # コメント key2: key3: null key4: - 1.1 - 2.2 key5: - key6: 3 key7: true - key6: 4 key7: false  TOML # コメント key1: \u0026quot;value1\u0026quot; # コメント [key2] key3: null key4: [1.1, 2.2] [[key5]] key6: 3 key7: true [[key5]] key6: 4 key7: false key8: 2018-11-19 key9: 2018-11-19T15:30:00+09:00 #日本時間15:30  "
    }
,
    {
        "ref": "https://koirand.github.io/blog/2018/textlint/",
        "title": "ブログの記事をtextlintで校正してみた",
        "section": "blog",
        "tags": null,
        "date" : "2018.11.11",
        "body": "このブログを書いていて、記事を公開してしまってから間抜けな誤字脱字に気づくことが度々あった。慌てて修正してPushして確認すると、また別のミスを見つけてしまって、何度も修正する羽目になる。個人ブログなので神経質になる必要はないがミスを見つけてしまうと何となく気になって修正してしまう。流石に虚しいので文章校正の仕組みを入れることにした。今はVimというテキストエディタで記事を書いているので、Vim上で文書校正をしてみた。\ntextlintのインストール 文章校正ツールである textlint インストール。ルールは種類が多すぎて正直どれを入れれば良いのか迷う。とりあえず無難そうな 技術文書向けのtextlintルールプリセットと、Webエンジニア向けの辞書 をインストールしてみた。\n$npm install -g textlint $npm install -g textlint-rule-preset-ja-technical-writing textlint-rule-spellcheck-tech-word  textlintの設定 ホームディレクトリに.textlintrcを作成する。デフォルトだと一文の長さが100文字を越えると警告が表示されるが、100文字だとURLなどを記載するとすぐに超えてしまうので200文字で上書きした。\n{ \u0026quot;rules\u0026quot;: { \u0026quot;preset-ja-technical-writing\u0026quot;: { \u0026quot;sentence-length\u0026quot;: { \u0026quot;max\u0026quot;: 200 } }, \u0026quot;spellcheck-tech-word\u0026quot;: true } }  Vimプラグインの導入 vimrcの設定例は以下。Lintエンジンの w0rp/ale と、ステータスラインをカスタマイズする itchyny/lightline.vimを使っている。\n\u0026quot; ale let g:ale_linters = { \\ 'markdown': ['textlint'] \\} \u0026quot; lightline let g:lightline = { \\ 'active': { \\ 'left': [ \\ ['mode', 'paste'], \\ ['readonly', 'filename', 'modified', 'ale'], \\ ] \\ }, \\ 'component_function': { \\ 'ale': 'ALEGetStatusLine' \\ } \\ }  結果 こんな感じで警告してくれるようになった。行番号の箇所に警告マークが表示され、そこにカーソルを合わせると警告の内容が一番下に表示される。また警告の総数がステータスラインに表示されている。便利。 "
    }
,
    {
        "ref": "https://koirand.github.io/blog/2018/pulp-search2/",
        "title": "HUGOテーマ(pulp)の全文検索をあいまい検索に変更した",
        "section": "blog",
        "tags": null,
        "date" : "2018.10.31",
        "body": "前回の記事「HUGOテーマ(pulp)に全文検索機能を付けた」でブログのテーマに全文検索機能を追加したが、HUGOのテーマとして配布し辛いという課題があった。lunr.js単体だけだとスペースで単語を区切らない言語での検索ができない。日本語も然り。トークナイズ用のプラグインを使えば正しく分かち書きされて検索できるようになるが、基本的にプラグインは特定の言語に特化したものなので、不特定な言語に対応できない。\nこの問題の解決方法としては以下２つが考えられる。\n N-gramを使う インデックスを使わない普通の検索にする  まずはN-gram方式を試してみた。lune.jsのトークナイズプラグインを探しても見つからなかったが、 Pull Request #63 olivernn/lunr.js　に紹介されているようにN-gramでの分割処理を実装してtokenizerに登録すればできる。\nvar myNgramTokenizer = function () { lunr.tokenizer = function (obj) { // ngram implementation } } idx.use(myNgramTokenizer)  検索クエリも同様にN-gramで分割処理をしてあげれば検索ができる。ただ単純なN-gramだけでは検索精度がいまひとつだった。第5回 N-gramのしくみ：検索エンジンを作る｜gihyo.jp … 技術評論社 にも記載されている通り、分割した文字列片がそれぞれバラバラの箇所でヒットしてしまう問題がある。検索精度を高めるためには、ヒットした文字列片の出現位置を見てあげる必要があるようだ。\nN-gramの実装はなかなか大変そうだったので、インデックスはあきらめてFuse.jsというあいまい検索のライブラリを使うことにした。Fuse.jsのデフォルトのオプションだと本文の先頭32文字しかヒットしないが、Hugo JS Searching with Fuse.js で紹介されている通り、tokenizeオプションを有効にすると全文検索可能になる。色々オプションを試して以下に落ち着いた。\nvar options = { shouldSort: true, tokenize: true, matchAllTokens: true, threshold: 0.3, minMatchCharLength: 5, keys: ['title', 'body'] }  検索精度は上々、不特定な言語でも問題ないはず。しかもあいまい検索なので若干スペルを間違えても検索ができる。ただしインデックスを使っていないのでパフォーマンスは悪い。前回と同じく Skycoin社のブログデータを使って試してみたら約150件の記事でも結構検索がカクついた。一旦はこれで運用するが、N-gramをもう少し頑張ったほうが良さそうだ。全文検索の道のりは思ったより険しい。\n2019-04-06 追記 やっぱりFuse.jsでのあいまい検索はパフォーマンス面が心配だったので、lunr.jsに戻してN-gramを実装する方式に変更した。jsファイルはこちら。\n pulp/search.js at master · koirand/pulp  インデックスを作成するときに使うトークナイザー(bigramTokeniser)を作成し、builder.tokenizer = bigramTokeniser の箇所でそれを利用している。\n検索時のクエリも同様に、N-gramでの分割関数(queryNgramSeparator)を作成し、検索時にlunrResult = lunrIndex.search(queryNgramSeparator(query))として使っている。これでN-gramが実現できた。全文検索の動作は このブログ で確認できる。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2018/pulp-search/",
        "title": "HUGOテーマ(pulp)に全文検索機能を付けた",
        "section": "blog",
        "tags": null,
        "date" : "2018.10.08",
        "body": "先日作ったpulpというHUGOのテーマに全文検索機能を付けてみた。タグやカテゴリで分類するのが個人的に面倒臭いと思ってしまうので、何もしなくても本文が検索できればいいなと思っていた。しっかり作るならElasticSearchやAlgoliaを使うんだろうけど、そうするとThemeとして配布しづらくなるし、個人用途としてはそこまでの機能はいらないのでフロントエンドだけで完結する方法を模索した。\nデモ 記事をJSONで取得できるようにする まずは、HugoでJSONを出力 | Celeumu を参考にしつつブログの記事をJSONで取得できるようにした。config.tomlに以下を追記すると、拡張子が.jsonのテンプレートをHUGOがテンプレートとして見てくれるようになった。\n[outputs] section = [\u0026quot;JSON\u0026quot;, \u0026quot;HTML\u0026quot;]  その上でlayouts/_default/list.json を作成すると、http://hostname/section/index.json といったURLでいとも簡単にセクションのデータをJSONで引っこ抜くことができるようになる。このブログの場合だと、https://koirand.github.io/blog/index.json からデータが取得できる。\n[{{ range $index, $page := .Pages }}{{ if ne $index 0 }},{{ end }} { \u0026quot;ref\u0026quot;: \u0026quot;{{ $page.Permalink }}\u0026quot;, \u0026quot;title\u0026quot;: {{ $page.Title | jsonify }}, \u0026quot;section\u0026quot;: \u0026quot;{{ $page.Section }}\u0026quot;, \u0026quot;date\u0026quot; : {{ $page.Date.Format \u0026quot;2006.01.02\u0026quot; | jsonify }}, \u0026quot;body\u0026quot;: {{ $page.Plain | jsonify }} } {{ end }}]  検索処理の実装 検索処理は hugo + gruntjs + lunrjs = search にlunr.jsを使ったサンプルコードを掲載してくれている人がいて、ほぼこのままで動いた。このサンプルコードをベースに以下のカスタマイズを加えた。具体的な変更内容は Add full-text search function by koirand · Pull Request #3 · koirand/pulp を参照。\n 検索結果に本文を表示 日本語での検索対応 キーワードのハイライト  検索結果に本文を表示 本文でキーワードにヒットした箇所の前後50文字を検索結果に表示した。複数のキーワードがスペース区切りで入力された場合は先頭のキーワードだけを使っている。また、タイトルのみにヒットした場合は、本文の先頭100文字を表示している。(恐らくもっと良いやり方がある)\n日本語での検索対応 標準のlunr.jsだとスペースを単語の区切りとみなすのか日本語を検索できないので、MihaiValentin/lunr-languages: A collection of languages stemmers and stopwords for Lunr Javascript library を使う必要がある。READMEには書いてないが、日本語に対応させる場合は同封されているtinyseg.jsも読み込む必要があった。\nキーワードのハイライト キーワードのハイライトは mark.js – JavaScript keyword highlight を使うと簡単だった。\n感想 たまに一部の単語が検索にヒットしないものの、概ねちゃんと検索できてるしサクサクで良い感じ。ページの数が増えてくるとパフォーマンス落ちたりするのかなと、試しに Skycoin社のブログデータ のリポジトリをクローンしてこのテーマを適用してみたが普通にサクサク動いたので個人サイトであれば大丈夫だろう。\n追記 HUGOテーマ(pulp)の全文検索をあいまい検索に変更した - Kazuki Koide\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2018/async-await/",
        "title": "Promiseとasync/awaitをちゃんと理解する",
        "section": "blog",
        "tags": null,
        "date" : "2018.10.01",
        "body": "JavaScriptといえばコールバックだが、最近は基本的に非同期関数はPromiseで実装してasync/awaitするのがトレンドらしい。もし使いたい関数がPromiseをサポートしていない場合は、自分でPromiseにラップするのが良いとのこと。この辺の知識が若干怪しかったので、実際に一からやってみた。\nコールバック方式 まずは、適当に従来のコールバック方式の非同期関数を書いてみる。\n// コールバック方式による非同期関数 function sum (x, y, callback) { setTimeout(() =\u0026gt; { if (typeof(x) !== 'number' || typeof(y) !== 'number') { callback(new Error('Invalid parameter!'), null) return } callback(null, x + y) }, 1000) } // 実行 sum(1, 2, (err, result) =\u0026gt; { if (err) { console.log(err) return } console.log('result:', result) })  sum関数は足し算をするだけの関数だが、1秒もかかってしまう。時間がかかるので非同期で動くようになっている\u0026hellip;ということにする。これを実行すると1秒後にresult: 3が出力される。引数に数値以外が指定された場合は以下のエラーが出力される。\nError: Invalid parameter! at Timeout.setTimeout [as _onTimeout] (/xxx/sandbox/nodejs/1.callback.js:4:16) at ontimeout (timers.js:498:11) at tryOnTimeout (timers.js:323:5) at Timer.listOnTimeout (timers.js:290:5)  Promise方式 次に、先ほどのsum関数をPromiseでラップする。\n// コールバック方式による非同期関数(さっきと同じもの) function sum (x, y, callback) { setTimeout(() =\u0026gt; { if (typeof(x) !== 'number' || typeof(y) !== 'number') { callback(new Error('Invalid parameter!'), null) return } callback(null, x + y) }, 1000) } // Promiseでラップ function promiseSum (x, y) { return new Promise((resolve, reject) =\u0026gt; { sum(x, y, (err, result) =\u0026gt; { if (err) { reject(err) return } resolve(result) }) }) } // 実行 promiseSum(1, 2).then(result =\u0026gt; { console.log('result:', result) }).catch(err =\u0026gt; { console.log(err) })  これがいわゆる「Promiseでラップする」ということ。これを実行しても先ほどと同様に1秒後にresult: 3が出力される。\nコールバック方式とPromise方式の比較 コールバック方式とPromise方式の実行コードを並べてみる。\n// コールバック方式 sum(1, 2, (err, result) =\u0026gt; { if (err) { console.log(err) return } console.log('result:', result) }) // Promise方式 promiseSum(1, 2).then(result =\u0026gt; { console.log('result:', result) }).catch(err =\u0026gt; { console.log(err) })  コールバック方式の場合は、関数の仕様に合わせて例外処理する必要がある。今回だと「コールバック関数の第一引数でエラーを受け取る」という仕様だ。それがPromiseでラップすることにより、どんな関数でも同じ方法(catch)で例外処理ができる。これはPromise方式のメリットだと感じた。可読性についてはどちらも変わらないと感じる。\n非同期関数を順番に実行する 次に、1から4の数字を全て足し合わせてみる。sum関数は2つの数字しか受け取れないので、1,2の足し算の結果と3を足して、その結果と4を足すという風に順次処理を行う。それぞれ以下のようなコードになる。\nコールバック方式 // コールバック方式による非同期関数 function sum (x, y, callback) { setTimeout(() =\u0026gt; { if (typeof(x) !== 'number' || typeof(y) !== 'number') { callback(new Error('Invalid parameter!'), null) return } callback(null, x + y) }, 1000) } // 実行 sum(1, 2, (err, result) =\u0026gt; { if (err) { console.log(err) return } sum(result, 3, (err, result) =\u0026gt; { if (err) { console.log(err) return } sum(result, 4, (err, result) =\u0026gt; { if (err) { console.log(err) return } console.log('result:', result) }) }) })  これを実行すると、足し算を３回行なっているので、３秒待ったあとにresult: 10が出力される。良くコールバック地獄やネスト地獄と言われて問題になっているが、この程度だとコールバック方式でもそこまで地獄感は感じない。地獄を味わっている人たちはどれくらい激しいネストをしているんだろうか。\nPromise方式 // コールバック方式による非同期関数 function sum (x, y, callback) { setTimeout(() =\u0026gt; { if (typeof(x) !== 'number' || typeof(y) !== 'number') { callback(new Error('Invalid parameter!'), null) return } callback(null, x + y) }, 1000) } // Promiseでラップ function promiseSum (x, y) { return new Promise((resolve, reject) =\u0026gt; { sum(x, y, (err, result) =\u0026gt; { if (err) { reject(err) return } resolve(result) }) }) } // 実行 promiseSum(1, 2).then(result =\u0026gt; { return promiseSum(result, 3) }).then(result =\u0026gt; { return promiseSum(result, 4) }).then(result =\u0026gt; { console.log('result:', result) }).catch(err =\u0026gt; { console.log(err) })  このコードを実行した場合も、３秒待ったあとにresult: 10が出力される。Promise方式だと、メソッドチェイン的な書き方で順番に実行でき、ネストは浅くなる。またPromise方式のもうひとつのメリットとして、try catch的な例外処理ができる。個別に例外を拾いたい場合は、以下のようにthen関数の第二引数に例外処理を書くことができる。\n// 数値以外を引数に指定して故意にエラーを発生させる promiseSum('x', 2).then(result =\u0026gt; { return promiseSum(result, 3) }, () =\u0026gt; { console.log('Error handling ...') }).then(result =\u0026gt; { return promiseSum(result, 4) }).then(result =\u0026gt; { console.log('result:', result) }).catch(err =\u0026gt; { console.log(err) })  この実行結果は以下となる。\u0026lsquo;Error handling\u0026hellip;\u0026lsquo;を出力したあとcatchに飛んでいるのがわかる。\nError handling ... Error: Invalid parameter! at Timeout.setTimeout [as _onTimeout] (/xxx/sandbox/nodejs/5.secencial-promise2.js:4:16) at ontimeout (timers.js:498:11) at tryOnTimeout (timers.js:323:5) at Timer.listOnTimeout (timers.js:290:5)  async/await方式 冒頭に書いた通り、最近はasync/awaitを使うのがトレンドだ。先ほどのPromise方式を async/awaitで書き換えると以下のコードになる。\n// コールバック方式による非同期関数 function sum (x, y, callback) { setTimeout(() =\u0026gt; { if (typeof(x) !== 'number' || typeof(y) !== 'number') { callback(new Error('Invalid parameter!'), null) return } callback(null, x + y) }, 1000) } // Promiseでラップ function promiseSum (x, y) { return new Promise((resolve, reject) =\u0026gt; { sum(x, y, (err, result) =\u0026gt; { if (err) { reject(err) return } resolve(result) }) }) } // 実行 (async () =\u0026gt; { try { let result; result = await promiseSum(1, 2) result = await promiseSum(result, 3) result = await promiseSum(result, 4) console.log('result:', result) } catch (err) { console.log(err) } })()  このコードを実行した場合も3秒待ったあとにresult: 10が出力される。Promise関数にawaitを付けることで処理の完了を待ってくれる。awaitはasyncを付けたfunctionの中だけで使えるというルールがあるので即時関数を使わないといけない点はイマイチだが、コールバックを使わずに同期処理が書けるので可読性が高いと感じる。普通にtry catchで例外処理できるのもポイント高い。ちなみに先ほどのように個別で例外を拾う場合は以下のようになる。\n(async () =\u0026gt; { try { let result; // 数値以外を引数に指定して故意にエラーを発生させる result = await promiseSum('x', 2).catch(err =\u0026gt; { console.log('Error handling ...') }) result = await promiseSum(result, 3) result = await promiseSum(result, 4) await console.log('result:', result) } catch (err) { console.log(err) } })()  このように、catchで個別に例外を拾うことができる。コールバック完全駆逐\u0026hellip;とまでは行かないようだ。\nasync/await方式でのテスト 昔は、非同期処理をテストするにはco-mocha等を使った気がするが、最近はmochaでasync/awaitが使えるようだ。以下のようなテストコードになる。\ndescribe('asunc/await test', () =\u0026gt; { it('1 + 2 = 3 ', async () =\u0026gt; { const result = await promiseSum(1, 2) assert.strictEqual(result, 3) }) it('数字以外が入力されるとエラー', async () =\u0026gt; { let f = () =\u0026gt; {} try { const result = await promiseSum('x', 2) } catch (err) { f = () =\u0026gt; {throw err} } finally { assert.throws(f, 'Invalid parameter!') } }) })  "
    }
,
    {
        "ref": "https://koirand.github.io/blog/2018/blog-auto-deploy/",
        "title": "ブログ(HUGO)のビルドとデプロイをCircleCIで自動化した",
        "section": "blog",
        "tags": null,
        "date" : "2018.09.30",
        "body": "このブログはHUGO(ヒューゴ)というブログ構築のためのフレームワークを使っている。HUGOで記事を作成する手順はWordPressなどのブログサイトとは異なり、PC上で静的なHTMLやCSSを生成してから、それをWebサーバにアップロードする必要がある。このブログの場合はGitHub Pagesを使っているので、Webサーバを用意しなくてもGitHubにPushすれば公開される仕組みになっているが、いずれにせよ一手間がかかる。これまでは Host on GitHub | Hugo を参考にサイトの生成とGitHubへのPushをシェルスクリプトで自動化していたが、それだったらCIツール使えばいいじゃんってことで、CircleCIでブログのデプロイを自動化することにした。\n自動化の過程 ブログの生成元となるリポジトリは非公開にしたかったのでBitbucketを使っている(GitHubはプライベートリポジトリが有料なので)。Hugo + Bitbucket + CircleCI で GitHub Pages に自動デプロイ | shotarok\u0026rsquo;s Tech Blog で既に同じ構成で自動化している人がいたのでこれを参考に構築した。ただし設定ファイルがCircleCI 1.0のものだったので、 Auto Deploy Hugo by Circle CI 2.0 - taikii blog を見ながらCircle CI 2.0を使った設定にした。Circle CIからGitHubにSSHアクセスする必要があるので、事前に鍵ペアを作成して、公開鍵をGitHubに秘密鍵をCircle CIにそれぞれ登録しておく必要があった。あとデプロイ結果をメールで確認するのが面倒だったので、CircleCI | Slack App ディレクトリ を使ってSlackで通知するようにした。\n.circleci/config.yml version: 2 jobs: build: docker: - image: cibuilds/hugo:latest environment: TZ: Asia/Tokyo working_directory: ~/blog steps: - checkout - run: name: \u0026quot;Setting for Git\u0026quot; command: | git config --global user.name \u0026quot;koirand\u0026quot; git config --global user.email \u0026quot;koirand.jp@gmail.com\u0026quot; - run: name: \u0026quot;Up to date themes\u0026quot; command: | git submodule sync git submodule update --init --recursive - run: name: \u0026quot;Get GitHub repository\u0026quot; command: git clone git@github.com:koirand/koirand.github.io.git public - run: name: \u0026quot;Build \u0026amp; Push\u0026quot; command: | hugo cd public git add . git commit -m \u0026quot;rebuilding site `date '+%Y-%m-%d'`\u0026quot; git push origin master  ビルド結果はこんな感じ。14秒くらいかかった。 2019-03-02 追記 GitHubのプライベートリポジトリが無料化されたので、ブログのリポジトリをGitHubに移行した\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2018/pulp/",
        "title": "HUGOのオリジナルテーマ(pulp)を作った",
        "section": "blog",
        "tags": null,
        "date" : "2018.09.29",
        "body": "このブログはHUGO(ヒューゴ)というブログ構築のためのフレームワークを使っている。HUGOはWordPressなどのブログサイトとは異なり、PC上で記事を書いてそれをHUGOを使って静的なHTMLやCSSを生成してくれる仕組みになっている。従ってWebサーバがあればどこでもアップロードして公開できる。\n The world’s fastest framework for building websites | Hugo\n 今回作ったのは、そのHUGOでHTMLやCSSを生成する際に使われるデザインテーマ。最近フロントエンドの画面を久しく作って無かったので、思い出しがてら作ってみることにした。「HUGO 自作テーマ」とかでググると作り方を解説してくれているサイトがいくつもあるし、公式ドキュメントに掲載されている動画が結構わかりやすいので、作り方については困らなかった。ちなみにこのブログはそのテーマを適用している。またURLを見ての通りGitHub Pagesで公開している。\nソースコード koirand/pulp: Pulp is a theme of Hugo framework for realize simple and readable personal blogs.\nスクリーンショット 特徴  色数を少なくして読みやすさを意識 表示媒体を選ばないデザイン 日本語対応 CSSやJSのカスタマイズ可能 コードのシンタックスハイライト対応   OGP,Twitter Card対応   一応SEOを意識して作ったけどまだ効果は不明 GoogleAnalyticsにも対応  今後やりたいこと  全文検索ができるようになるといいかな しばらく自分のブログで運用して、問題なさそうだったらHUGO公式サイトに掲載してもらう予定  "
    }
,
    {
        "ref": "https://koirand.github.io/blog/2018/line-login-with-springboot/",
        "title": "SpringbootでLINEのOAuth2ログインを試した",
        "section": "blog",
        "tags": null,
        "date" : "2018.09.24",
        "body": "タイトルの通り、いわゆる「LINEでログイン」を試してみた時のメモ。\nソースコード koirand/springboot-line-login: LINE login with Springboot Sample\nOAuth2での認証方法  第1回：Spring Security 5でサポートされるOAuth 2.0 LoginをSpring Bootで使ってみる\n 主にこの記事にOAuth2クライアントの実装を行った。OAuth2でログインするためには、spring-security-oauth2-clientモジュールを使う。OpenID Connectでの認証を行う場合は加えてspring-security-oauth2-joseが必要になるが、今回はOAuth2のみなので入れなかった。\n\u0026lt;!-- OAuth2 authentication--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.security\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-security-oauth2-client\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  application.properties に以下の通りLINE用の設定を記載する。clientIDとclientSeretはLINE Developersで取得したものに書き換える。\n# Settings for LINE login spring.security.oauth2.client.registration.line.clientId={Set your client id} spring.security.oauth2.client.registration.line.clientSecret={Set your client secret} spring.security.oauth2.client.registration.line.authorizationGrantType=authorization_code spring.security.oauth2.client.registration.line.redirectUriTemplate={baseUrl}/login/oauth2/code/{registrationId} spring.security.oauth2.client.registration.line.scope=profile spring.security.oauth2.client.registration.line.clientName=LINE spring.security.oauth2.client.provider.line.authorizationUri=https://access.line.me/oauth2/v2.1/authorize spring.security.oauth2.client.provider.line.tokenUri=https://api.line.me/oauth2/v2.1/token spring.security.oauth2.client.provider.line.jwkSetUri=https://api.line.me/oauth2/v2.1/verify spring.security.oauth2.client.provider.line.userInfoUri=https://api.line.me/v2/profile spring.security.oauth2.client.provider.line.userNameAttribute=userId  Spring Securtyでは、GitHub、Google、Facebook、Oktaの４つのプロバイダに関しては、各設定値の初期値が既に設定されている。なのでapploication.propatiesにはclientIdとclientSecretだけを設定すれば動作する。例えばGitHubを追加するなら、以下の設定を追加するだけで良い。\n# Settings for GitHub login spring.security.oauth2.client.registration.github.clientId={Set your client id} spring.security.oauth2.client.registration.github.clientSecret={Set your client secret}  LINEは初期値が設定されていないので全ての設定が必要になる。scopeの種類や、各エンドポイントのURI、APIの仕様などは以下を参照。\n  ウェブアプリにLINEログインを組み込む（LINEログインv2） Social API v2.0リファレンス   LINEの場合、ユーザープロファイルを取得するためのAPI(userInfoUriで指定しているエンドポイント)のレスポンスが以下のようなJSONになる。従ってuserNameAttributeには、ユーザー名に該当するuserIdを指定している。\n{ \u0026quot;userId\u0026quot;:\u0026quot;U4af4980629...\u0026quot;, \u0026quot;displayName\u0026quot;:\u0026quot;Brown\u0026quot;, \u0026quot;pictureUrl\u0026quot;:\u0026quot;https://example.com/abcdefghijklmn\u0026quot;, \u0026quot;statusMessage\u0026quot;:\u0026quot;Hello, LINE!\u0026quot; }  ちなみにSpringboot v2.1.0.M2より古いバージョンは、userNameAttributeが反映されないので注意されたし。\n UserNameAttribute for custom OAuth2 provider by thiagohirata · Pull Request #10672 · spring-projects/spring-boot\n コントローラーのメソッドの引数に@AuthenticationPrincipalを指定することで、認証済みのユーザーの情報を受け取ることができるようになる。ここではシンプルにユーザー情報をレスポンスボディにそのままセットしている。\npackage com.example; import org.springframework.security.core.annotation.AuthenticationPrincipal; import org.springframework.security.oauth2.core.user.OAuth2User; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.bind.annotation.GetMapping; @RestController public class SampleController { @GetMapping(\u0026quot;/\u0026quot;) public Object index(@AuthenticationPrincipal OAuth2User oauth2User) { return oauth2User; } }  http://localhost:8080にアクセスすると、未認証の場合は、/loginにリダイレクトされる。/loginエンドポイントを実装していなければ、Spring SecurityのDefaultLoginPageGeneratingFilterが以下のログイン画面を自動生成する仕組みになっている。\nこの例ではLINEのみだが、複数のプロバイダを設定した場合はそれぞれのリンクが表示される。LINEリンクをクリックすると、http://localhost:8080/oauth2/authorization/lineに遷移する。自前のログインページを作成する場合は同じようにこのURIへ遷移するリンクなりボタンを作成すれば良い。http://localhost:8080/oauth2/authorization/lineにアクセスすると、そこから更にauthorizationUriで設定したhttps://access.line.me/oauth2/v2.1/authorizeへリダイレクトしてLINEの認証画面が表示される。認証に成功すると、http://localhost:8080/login/oauth2/code/lineへリダイレクトし、そして最初にアクセスしたhttp://localhost:8080へリダイレクトする。まとめると以下のようなページ遷移になる。\nhttp://localhost:8080 ↓ リダイレクト http://localhost:8080/login ↓ リンクをクリック http://localhost:8080/oauth2/authorization/line ↓ リダイレクト https://access.line.me/oauth2/v2.1/authorize ↓ 認証成功 http://localhost:8080/login/oauth2/code/line ↓ リダイレクト http://localhost:8080  なお、認証成功時のリダイレクト先のURLについては、LINE Developer画面上でホワイトリストに設定しておく必要がある。LINE以外の他のプロバイダでも同様。\n別の実装方法 LINE Engieeringブログで、spring-boot-starter-securityとspring-security-oauthを使った実現方法が紹介されていたが、この方法だとOAuth2プロバイダが１つしか指定できなかったので採用しなかった。実際はLINEだけでなくGoogleやFacebookと併用したいケースが多いのではないだろうか。\n Spring Security + 設定ファイルで始める LINE との ID Federation : LINE Engineering Blog\n "
    }
,
    {
        "ref": "https://koirand.github.io/blog/2018/tokyo-metro-vim/",
        "title": "東京メトロ配色のVimカラースキームを作った",
        "section": "blog",
        "tags": null,
        "date" : "2018.09.24",
        "body": "VimConf2017でcocoponさんがicebergというカラースキーマを作った時の話をしているのを聞いて自分もオリジナルのカラースキーマを作ってみたいと思った。東京に住んでるし東京をテーマとしたカラーススキームを作れないかとちょっと考えたものの、色彩センスに自信がなかったので、東京メトロのテーマカラーを使ってみることにした。\nソースコード koirand/tokyo-metro.vim: Vim color scheme using theme color of Tokyo Metro lines.\n作成過程 まずはPhotoshopで各路線のテーマカラーを並べた。\nダークモード派なので黒背景。思ったより見やすい印象を得た。さすが偉い人が考えた配色である。ただ千代田線と南北線の色が似ていて使い分けが難しい。カラースキームの自作方法についてWebで調べて見たところ、設定項目数が多くなかなか大変そうだったので、iceberg.vim をForkしてカスタマイズすることにした。README に記載されているが、カラースキーマを半自動生成する仕組みが提供されているのでこれを使用した。具体的な手順としては以下の通り。\n cocopon/pgmnt プラグインをVimにインストール。 src/template.vim にあるテンプレートを変更。 autoload/iceberg/palette/dark.vim をコピーして、autoload/tokyometro/palette/dark.vim を作成して、配色を変更。:source % してVimに読み込む。 src/iceberg.vim をコピーして src/tokyo-metro.vim を作成して、色と対象オブジェクトのマッピングを少し調整。cd %:hしてカレントディレクトリを変更してから:source %すると、color/tokyo-metro.vim が生成される。  できたのがこちら。 tokyo-metro - Vim Colors\n濃紺の背景は東京の夜景っぽくて違和感なかったのでオリジナルのままにした。このサンプルでは表示されてないけど、言語によっては演算子が副都心線の茶色になるはず。それから丸ノ内線については、やは赤はエラー感が強く出てしまい違和感が拭えなかったので、丸の内線ユーザーには申し訳ないが素直にエラー系の色を当てることにした。\n作ってみた感想 オリジナルカラースキームができてうれしい。cocopon/pgmnt プラグインが良く作られている。色をHSLで指定できるので、微妙な色の調整がやりやすそうに感じた。今回テーマカラーありきだったので、あまり配色には悩まなかったが、次回作るときは配色に拘ってみたい。\n"
    }
,
    {
        "ref": "https://koirand.github.io/blog/2018/post-test/",
        "title": "Post Test",
        "section": "blog",
        "tags": null,
        "date" : "2018.09.23",
        "body": "見出し1（h1） 見出し1（h1） 見出し2（h2) 見出し2（h2） 見出し3 見出し4 見出し5 見出し6  ここは段落です。♪もーもたろさん もーもたーろさん おっこしーにつっけたーちーびまーるこー\nここは段落です。\n↑半角スペース2個で強制改行しています。\n♪もーもたろさん もーもたーろさん おっこしーにつっけたーちーんあーなごー\n 強い強調（strong）です。 これも強い強調です。 \u0026lt;strong\u0026gt;strongタグです。\u0026lt;/strong\u0026gt; 強調（em）です。 これも強調です。 斜体の\u0026lt;em\u0026gt;タグになります。 強調斜体です。 強調斜体です。 \u0026lt;strong\u0026gt;＋\u0026lt;em\u0026gt;タグになります。   引用（Blockquote）です\n   引用のネストです\n   上に一行空けないとネストのままです\n 引用（Blockquote）の中にはMarkdown要素を入れられます\n 見出し  数字リスト 数字リスト   エスケープ文字 *アスタリスクをバックスラッシュでエスケープ*\n## 見出しハッシュ文字をエスケープ\nHTMLタグをバックスラッシュでエスケープ→（\u0026lt;p\u0026gt;）\nHTMLをバッククォートでインラインコード→（\u0026lt;p\u0026gt;）\n水平線（\u0026lt;hr\u0026gt;）各種 アスタリスク3個半角スペース空けて\n アスタリスク3個以上\n ハイフン半角スペース空けて\n 続けてハイフン3個以上\n リスト  ハイフン箇条書きリスト   プラス箇条書きリスト   米印箇条書きリスト  二階層め・箇条書きリスト  三階層め・箇条書きリスト 四階層め・箇条書きリスト       箇条書きリスト    番号付きリスト  二階層め・番号付きリスト1 二階層め・番号付きリスト2   番号付きリスト2  二階層め・番号付きリスト1  三階層め・番号付きリスト1 三階層め・番号付きリスト2 四階層め・番号付きリスト1   二階層め・番号付きリスト2   番号付きリスト3   定義リストタイトル 定義リスト要素1 定義リスト要素2 定義リスト要素3  コードブロック バッククォート or 半角チルダ3個でくくります。 ###ここにはMarkdown書式は効きません /* コメント */ testtest // コメント  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026quot;X-UA-Compatible\u0026quot; content=\u0026quot;IE=edge\u0026quot;\u0026gt; \u0026lt;title\u0026gt;ニョロニョロ囲みhtml\u0026lt;/title\u0026gt; /* コメント */  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026quot;X-UA-Compatible\u0026quot; content=\u0026quot;IE=edge\u0026quot;\u0026gt; \u0026lt;title\u0026gt;バッククォート囲みhtml\u0026lt;/title\u0026gt;  body { display: none; } /* バッククォート囲みcss */ // コメント  // 先頭に半角スペース4つでcode囲い \u0026lt;?php if (is_tag()){ $posts = query_posts($query_string . '\u0026amp;showposts=20'); } ?\u0026gt;  バッククォート1個ずつで囲むとインラインのコード（\u0026lt;code\u0026gt;\u0026lt;/code\u0026gt;）です。body { visibility: hidden; }\nリンク markdownでテキストリンク WIRED.jp\n\u0026lt;カッコ\u0026gt;でくくってリンク http://wired.jp/\n定義参照リンクです。SNSには [Twitter] 1 や [Facebook] 2 や [Google+] 3 などがあります。\n画像 table    Left align Right align Center align     This This This   column column column   will will will   be be be   left right center   aligned aligned aligned    （Kobitoのヘルプmdから拝借しました）\nGFM リンク URLそのまま貼り付け http://wired.jp/\n段落中の改行 ここは段落です。 ↑returnで改行しています。 ♪もーもたろさん もーもたーろさん おっこしーにつっけたーちー○○ー○○ー\nコードブロック バッククォートの開始囲みに続けて拡張子でシンタックスハイライト\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026quot;X-UA-Compatible\u0026quot; content=\u0026quot;IE=edge\u0026quot;\u0026gt; \u0026lt;title\u0026gt;バッククォート囲みに拡張子付きhtml\u0026lt;/title\u0026gt; /* コメント */  body { display: none; } /* コメント */  \u0026lt;?php if (is_tag()){ $posts = query_posts($query_string . '\u0026amp;showposts=20'); } ?\u0026gt;  取り消し線 取り消し線（GFM記法）\nsタグです。単語中のアンダースコアの無効 GitHub_Flavored_Markdown_test_test\ntasklist  task1 task2 completed task   from Markdown記法 表示確認用サンプル - Qiita\n"
    }
]
